[{"post_change_time":1546000981993,"post_content":"<p>Hi, good moring</p>\n<p>I think now it is the time to put in pagination and build more static views of posts organized by dates.</p>"},{"post_change_time":1545966102441,"post_content":"<p>Ok, so maybe I am being stingy with the data - I figure the authors would not want me posting the baseball data publicly, the right thing would be to contact them and find out if they'd be ok with that.  Maybe I will do that.</p>  \n\n<p>Anyway, I have added any intermediate and final files related to extracting the baseball data to the .gitignore for the <a href='https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions.git'>https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions.git</a> repository.</p>"},{"post_change_time":1545965881932,"post_content":"<p>Ok, tesseract worked pretty nicely to get the baseball data to text format, just had to remove some blank lines after copy/pasting into a new file.  The revision from StackOverflow is: <a href='https://stackoverflow.com/posts/26478719/revisions'>https://stackoverflow.com/posts/26478719/revisions</a> by StackOverflow from October 21, 2014.  I am afraid to mention the user's name here, publicly.  If I was citing for a research paper, I would.</p>"},{"post_change_time":1545964288958,"post_content":"<p>ok, installed tesseract, seems like for dealing with this particular pdf, gost script is better than converting the pdf to tiff before ocr'ing it.  The syntax for the ghost script command is here.  The command line options are inscrutable to me: <a href='https://stackoverflow.com/posts/75567/revisions'>https://stackoverflow.com/posts/75567/revisions</a>.  I used the answer with edit by StackOverflow user BAR, from February 11th, 2016.<p>\n\n<p>While I'm wating for the ocr to finish, here's the command:</p>\n<pre>gs -q -dNOPAUSE -sDEVICE=tiffg4 -sOutputFile=a.tif foo.pdf -c quit</pre>\n<p>OK, OCR's done, let's get some data.</p>  "},{"post_change_time":1545963063938,"post_content":"<p>Ok, so </p>\n<ol>\n  <li>I really do not find much for sample proportion doing a google search.  I am thinking it probably goes by a different name, but it seems to me to be the ratio of the count of elements of a particular class in a distribution, divided by one less than the the number of elements in the distribution that are not of the class as the numerator.</li>\n<li>We've got to get mathjax working here</li>\n<li>We're goint to OCR <a href='http://www.stat.wmich.edu/s160/hcopy/book.pdf'>http://www.stat.wmich.edu/s160/hcopy/book.pdf</a>Since it seems like a scanned pdf and:\n<li>  <ol>\n    <li>They want us to work with this big dataset</li>\n    <li>If I can't copy &amp; paste it then it will take too long to deal with.</li>\n  </ol>\n</li>\n</ol>\n\n<p>for this perpose, I just installed tesseract. I wonder if that is still the best choice. </p>"},{"post_change_time":1545953960164,"post_content":"The test 3"},{"post_change_time":1545948962808,"post_content":"<p>Ok, starting on some statistics questions on a slow afternoon.  I was busy with mundane distractions this morning.</p>\n\n<p>I just realized, the easy way to use mathjaxs here is to use jupyter. Perhaps I will link to jupyter notebooks from here, or save them as html and then link to them, or maybe even frame them.</p>\n\n<p>Ok, off to another holiday dinner with family.</p>"},{"post_change_time":1545876145871,"post_content":"<p>well, I just updated this blog posting page, now the common areas are coming in double.  After finishing the header, footer and left column, I am wondering about the best way to do the center content.  If I leave this the way it is, we will have an html fragment referring to javascript components.  If we do that then we will have html files calling javascript and it won't be obvious where the source code for that javascript is.</p>"},{"post_change_time":1545875012092,"post_content":"<p>ok, I started a new repo at: <a href='https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions.git'>https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions.git</a> for working through problems in this statistics book.  I think it is pretty basic but it should fill in some gaps.</p>\n\n<p>Now I am working on factoring out common code.  Let's make sure the home page still loads</p>."},{"post_change_time":1545869380184,"post_content":"<p>Well good morning.</p>\n<p>That was easy.  We have submitted a csv to Kaggle.  However, it won't win, since it's nowhere near 100%</p>\n\n<p>So, what to study today? More of what is trendy? The latest in AI research?</p>\n\n<p>I've started checking recent submissions on archiv.  Looks like some new interesting material on neural modulation.</p>\n\n<p>I know that I need to tune hyperparameters for the titanic code - here seems like a decent treatise on the subject: <a href='https://github.com/TLESORT/Generative_Continual_Learning'>https://github.com/TLESORT/Generative_Continual_Learning</a> But I want something easy... like grid search for sklearn.</p>\n\n<p>Oh, and on another subject, the keras github repo has a lot of examples, and for that matter, the tensorflow one probably does, too.  Anyway, I'm setting up a pycharm project for those.</p>\n\n<p>I am starting to think I should go through the exercise of building tensorflow from source again, but... it is a lot of commands & I don't feel quite up to it at the moment.</p>\n\n<p>Or, perhaps what I should really do is find out how to start up a GPU instance on Google compute cloud.  The last time I tried I ran into some error about my quota.</p>\n\n<p>Ok, just made links on index page dynamic.  eventually this can be a one page site, using the react design pattern.  I'd rather not get into using all those libraries at this time.</p>"},{"post_change_time":1545830537578,"post_content":"<p>Still working on this titanic code, the next things is to have it generate a submission output for test.csv.  I'll create a to-do project and put that on there.</p>\n\n<p>Aside: Looks like \"m-x eshell\", or \"m-x ansi-term,\" are better alternatives to, \"m-x shell.</p>\n\n<p>Wow, ok, it looks like people can get 100% accuracy on the titanic data.</p>\n\n<p>I found random forest with default parameters gets 80%, and predicting by gender gets about 78%.</p>\n\n"},{"post_change_time":1545792925463,"post_content":"<p>back to titanic exercise</p>\n\n<p>df.describe gives NaN for unique counts when I think it shouldn't be.</p>\n\n<p>I've resorted to awk, but perhaps I should do some research.</p>\n\n<p>Nothing comes up  immediately, using google.</p>\n\n<pre> awk -F ... | sort | unique </pre>\n\n<p>Is working a lot better for me.</p>\n\n<p>I need to stop and get my IRC channel stuff working again. Ok, that really wasn't so bad.</p>\n\n<p>ok, just found something interesting.  I just did two commits to the <a href='https://github.com/jhancock1975/kaggle-titanic</a>https://github.com/jhancock1975/kaggle-titanic</a> repository.  The URI for cloning this repository is: <a href='https://github.com/jhancock1975/kaggle-titanic.git'>https://github.com/jhancock1975/kaggle-titanic.git</a>.  The two commits are:\n<ul>\n  <li><a href='https://github.com/jhancock1975/kaggle-titanic/blob/30b0b244674845edf3796f84f68ff7053c44116e/nn/titanic-nn.py'>30b0b244674845edf3796f84f68ff7053c44116e a Keras dense neural network to train on the Kaggle Titanic starter contest data (train.csv) that uses categorical cross-entropy that gets about 0.78 accuracy, and</a></li>\n  <li><a href='https://github.com/jhancock1975/kaggle-titanic/blob/1a609d76d9b70e9bae9199b1b5e927e1ca1b9c16/nn/titanic-nn.py'>1a609d76d9b70e9bae9199b1b5e927e1ca1b9c16 a Keras dense neural network to train on the same data that uses binary cross-entropy that gets about 0.38 accuracy.  Moreover, the accuracy does not seem to change all that much across epochs.</a></li>\n</ul>\n<p>I do not know why accuracy is so much lower for just changing the type of entropy, but I think it's a noteworthy result."},{"post_change_time":1545752512857,"post_content":"<p>Good morning.</p>\n\n<p>Working on the titanic starter-kaggle<p>\n\n<p>I wrote a baseline classifier using random forest that gets about 80%</p>\n\n<p>I wonder if it is about time to start using math jax here for equation typesetting.</p>\n\n<p>Dropout, after activation.  Makes sense, right?</p>\n\n<p>Wow.  I thought I would look at the origin of dropout in the paper: <a href=\"http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\">http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf</a>.  This paper is 30 pages.</p>\n\n<p>On a side note, I'm trying out using emacs for my shell. At work we are into using windows with split screens for our shells, emacs seems to be really good for having multiple shells with shells underneath, and is smarter about selecting with the mouse.  Tmux I can have windows underneath, but it feels sluggish, and the mouse selects across split windows.  Screen I can't find a way to have shell windows that are not visible if I am using a split screen, and selecting with the mouse selects on all visible shells.  Emacs suffers from none of these.  The only drawback seems that it doesn't play well with less out of the box.</p>\n\n<p>Oh, you know when you learn about something, and feel like a dolt because knowing what you just learned would have saved you a lot of time and anguish, and everyone else probably already knows this?  I just had one of those moments.  With vi on a mac, if I press option and then &lt;shift&gt;-: it's the same as typing &lt;esc&gt; &lt;shift&gt;-:. <p>\n\n<p>Ok, on data cleaning portion now, for random forest I just dropped anything not numerical, but now I want to keep more values.</p>\n\n<p>Going for a jog now, I'll post more later.</p>"},{"post_change_time":1545744381959,"post_content":"<p>For the blackjack challenge, I do not want to get too far into it because it is a toy problem.  From what I read in the discussion, and I am agnostic on the result, people are writing that ~ 43% is the max.  This gets about 41%:</p>\n\n<pre>\n if (player_total < 17) and (dealer_card_val <17):\n        return True\n</pre>\n\n<p>And here is my brute force for finding a linear combination of the parameters that finds the optimum pricing strategy:\n\n<pre>\nimport sys\nloop_range=range(-1,1)\nmax_result = sys.float_info.min\nbest_params = {'x1': 0, 'x2':0, 'x3':0, 'x4':0}\nfor x1 in loop_range:\n    for x2 in loop_range:\n        for x3 in loop_range:\n            for x4 in loop_range:\n                def get_pricing_function():\n                    def pricing_function(days_left, tickets_left, demand_level):\n                        \"\"\"Sample pricing function\"\"\"\n                        price = x1*days_left + x2*tickets_left + x3*demand_level + x4\n                        return price\n                    return pricing_function\n                result=simulate_revenue(days_left=7, tickets_left=50, pricing_function=get_pricing_function(), verbose=True)\n                if (result > max_result):\n                    max_result = result\n                    best_params = {'x1': x1, 'x2':x2, 'x3':x3, 'x4':x4}\n                print(\"result = %s\" % result)\nprint('max_result = %f' % max_result)               \nprint('best_params = %s' % best_params)\n</pre>\n\n<p>Ok, so what next?  I think it is important to keep abreast of what is happening on kaggle, what is trending on Github, Arxiv, ACM, etc.</p>\n\n<p>I'm trying a kaggle beginner contest now.  I think Kaggle competitions are probably one of the best ways to learn more about doing machine learning programming, but I am afraid of getting really sucked into some contest where I spend all my time working on that and nothing else.  This is what happen the last contest I was in at work.</p>\n\n<p>So anyway, I'm going to try doing a submission with the kaggle api now.  Looks like I can install it with just a pip package.</p>"},{"post_change_time":1545678557687,"post_content":"<p>For the blackjack challenge, I do not want to get too far into it because it is a toy problem.  From what I read in the discussion, and I am agnostic on the result, people are writing that ~ 43% is the max.  This gets about 41%:</p>\n\n<pre>\n if (player_total < 17) and (dealer_card_val <17):\n        return True\n</pre>\n\n<p>And here is my brute force for finding a linear combination of the parameters that finds the optimum pricing strategy:\n\n<pre>\nimport sys\nloop_range=range(-1,1)\nmax_result = sys.float_info.min\nbest_params = {'x1': 0, 'x2':0, 'x3':0, 'x4':0}\nfor x1 in loop_range:\n    for x2 in loop_range:\n        for x3 in loop_range:\n            for x4 in loop_range:\n                def get_pricing_function():\n                    def pricing_function(days_left, tickets_left, demand_level):\n                        \"\"\"Sample pricing function\"\"\"\n                        price = x1*days_left + x2*tickets_left + x3*demand_level + x4\n                        return price\n                    return pricing_function\n                result=simulate_revenue(days_left=7, tickets_left=50, pricing_function=get_pricing_function(), verbose=True)\n                if (result > max_result):\n                    max_result = result\n                    best_params = {'x1': x1, 'x2':x2, 'x3':x3, 'x4':x4}\n                print(\"result = %s\" % result)\nprint('max_result = %f' % max_result)               \nprint('best_params = %s' % best_params)\n</pre>\n\n<p>Ok, so what next?  I think it is important to keep abreast of what is happening on kaggle, what is trending on Github, Arxiv, ACM, etc.</p>\n\n<p>I'm trying a kaggle beginner contest now.  I think Kaggle competitions are probably one of the best ways to learn more about doing machine learning programming, but I am afraid of getting really sucked into some contest where I spend all my time working on that and nothing else.  This is what happen the last contest I was in at work.</p>\n\n<p>So anyway, I'm going to try doing a submission with the kaggle api now.  Looks like I can install it with just a pip package.</p>"},{"post_change_time":1545664785111,"post_content":"<p> Good morning.</p>\n\n<p>Ok, the command pallet is important in Jupyter - initially I saw something that said to use ctrl-shift-p.   That won't work in Firefox, so a little Googling found ctrl-shift-f, which works in Firefox.</p>\n\n<p>Speaking of Firefox, I'm really happy with how it has been working, and I have been using it for a Browser for about a week now.</p>\n\n<p>ok, Matrix factorization</p>\n\n<p>I'm not seeing how factorization comes into play in this section.  The main thrust of it seems to be that we take the dot product of the vectors that result from the embedding.  Maybe that is somehow equivalent to using factors of the embedding matrices, but I do not understand how that is.</p>\n\n<p>Gensim looks very interesting, especially for using an embedding after it is learned.</p>\n\n<p>Also good introduction to t-SNE in embeddings.  I remember plotting t-SNE plots from embeddings before, but I don't remember if I was using the scikit learn library for that.  This is a useful tool.</p>\n\n<p>Onto the micro challenges</p>\n\n<p>First one is about blackjack. </p>\n\n<p>On a side note, I just added a verification tag to this site's index.html page, we will see how results show up on Google.</p>\n\n<p>I would like to find a way to have the Google search engine index all of the postings on this.</p>"},{"post_change_time":1545625776016,"post_content":"<p>Well, apparently the post from the car did not make it</p>\n\n<p>I wonder why.</p>\n\n<p>On to the next training, which is: embeddings.</p>\n\n<p>The embeddings tutorial has important information on how to use the Keras functional API for models that have more than one input, which is probably a key piece I was missing when I was trying to add inputs to my GAN experiments earlier.</p>"},{"post_change_time":1545623706097,"post_content":"<p>Ok, back to kaggle tutorials, or doing something with the wife.   I hear a lot of exasperation in the background.</p>\n\n<p>More good stuff in the Machine Learning Explainability course, partial dependency plots, and the Python PDPBox  library.</p>\n\n<p>The partial dependence plots for single variables in the for loop are not showing for me.  We could google that, but why spend time fixing these workbooks?  I'm not inclined to do that here.</p>\n\n<p>I really like the Machine Learning Explainability course, but it packs a lot in and it deserves a few passes, or more reading on the same topics from another source.</p>"},{"post_change_time":1545583818800,"post_content":"<p> Well, it's been a busy morning, blogwise. This blog software is now a minimum viable product; people can use this with their github account, and they will not use any of their Github API quota when people view their index page.</p>\n\n<p>The next thing will be to refactor these pages so we are serving the common elements using fetch instead of serving duplicated code, which is what we are doing now. </p>"},{"post_change_time":1545582653622,"post_content":"test 1130 1 abcd"},{"post_change_time":1545581903707,"post_content":"I put a cool effect"},{"post_change_time":1545580657126,"post_content":"test 1057 abcd"},{"post_change_time":1545579681554,"post_content":"test abcd 1041"},{"post_change_time":1545579043550,"post_content":"test 1030 2 abcd"},{"post_change_time":1545579023595,"post_content":"test 1030 abcd 1"},{"post_change_time":1545579010646,"post_content":"test 1029 1 abcd"},{"post_change_time":1545572007340,"post_content":"test 0833 1"},{"post_change_time":1545571819059,"post_content":"test 0830 1"},{"post_change_time":1545570830892,"post_content":"test 0813 1"},{"post_change_time":1545570605941,"post_content":"<p> well the blog is in a state of flux now, I started refactoring so that we can save posts to a single file that gets loaded when the home page loads.</p>\n\n<p>It's about half done, but I think the changes are local thus far.</p>\n\n<p>We're on to the last section in the R tutorials now, to take a little break from coding.</p>\n\n<p>I want to let my subconscious work on the refactoring for a bit.</p>\n\n<p>Wow, ok, at least in the Kaggle Jupyter notebooks with the R kernel, hep(function_name) returns a really nicely formatted help page.  For example:\n\n<pre>help(str)</pre>\n\n<p><i>How quickly I forget!<i> In R str is for <i>structure</i>.  Nothing to do with <i>str</i>ings.</p>\n\n<p>Hmmm. I notice we have gone from mean absolute error to root mean squared error.  </p>\n\n<p>Ok, probably way too long to have found this out, but the best way to deal with these Kaggle notebooks when the commit button stops working is to use the cloud buttons to download and upload the notebook.  Saves time over copying &amp; pasting the cells...who would do that?</p>\n\n<p>I'm going to go out of order for the last three modules - I'm going to do this Machine Learning Explainability first, then embeddings, then the micro challenges.</p>\n\n<p>This is the first I've heard of Machine Learning Explainability, and the first my browser's spell checker has heard of it as well, it would seem...</p>\n\n<p>Wow, the machine learning explainability track starts off with some great information.  Intuitive and ... one of those things I feel like I should have thought of... shuffle a column of the validation data, and see if performance goes down.  If it does, we know the column is important.  Brilliant!</p>\n\n"},{"post_change_time":1545520348147,"post_content":"test 1812 1"},{"post_change_time":1545520290387,"post_content":"test 1811 1"},{"post_change_time":1545519992677,"post_content":"test 1806 1"},{"post_change_time":1545519919305,"post_content":"test 1805 1"},{"post_change_time":1545519830608,"post_content":"test 1803 1"},{"post_change_time":1545519076188,"post_content":"test 1751 1"},{"post_change_time":1545519024067,"post_content":"test 1750"},{"post_change_time":1545518985601,"post_content":"test 1749 1"},{"post_change_time":1545518857705,"post_content":"test 1747 1"},{"post_change_time":1545518807800,"post_content":"test 1746 1"},{"post_change_time":1545518764226,"post_content":"test 1754 1"},{"post_change_time":1545518664492,"post_content":"test 1744 1"},{"post_change_time":1545518469901,"post_content":"test 1741 1"},{"post_change_time":1545518386391,"post_content":"test post 1739"},{"post_change_time":1545518316040,"post_content":"test 1738 1"},{"post_change_time":1545518210752,"post_content":"test 1736 2"},{"post_change_time":1545518078245,"post_content":"test 1734"},{"post_change_time":1545517963941,"post_content":"test post 1732"},{"post_change_time":1545517812892,"post_content":"test 1730"},{"post_change_time":1545517722855,"post_content":"test 1728"},{"post_change_time":1545517675766,"post_content":"test 1727 2"},{"post_change_time":1545517648927,"post_content":"test post 1727"},{"post_change_time":1545517563951,"post_content":"test 1725"},{"post_change_time":1545517367264,"post_content":"test 1722"},{"post_change_time":1545517290211,"post_content":"test post 1721"},{"post_change_time":1545515992322,"post_content":"test 1659"},{"post_change_time":1545515928849,"post_content":"test 1658"},{"post_change_time":1545515756652,"post_content":"test 1655"},{"post_change_time":1545515691487,"post_content":"test 1654"},{"post_change_time":1545515587446,"post_content":"test 1653"},{"post_change_time":1545515499535,"post_content":"test 1651"},{"post_change_time":1545515431726,"post_content":"test post 1650"},{"post_change_time":1545515101799,"post_content":"test post 1644"},{"post_change_time":1545514960555,"post_content":"test post 1642"},{"post_change_time":1545514844168,"post_content":"test post 2 1640"},{"post_change_time":1545514811204,"post_content":"test post 1640"},{"post_change_time":1545514736433,"post_content":"test post 1638"},{"post_change_time":1545514588510,"post_content":"test blog post"},{"post_change_time":1545499000802,"post_content":"testing .then"},{"post_change_time":1545498390141,"post_content":"testing clear text box after clicking on save post"},{"post_change_time":1545498037581,"post_content":"<p>Well, the xgboost tutorial is ok but there were a lot of things I could not get to work, so I want to study those in detail from another source.</p>\n\n<p>Especially the function for plotting a decision tree- </p>\n<pre>xgb.plot.multi.trees</pre>"},{"post_change_time":1545497655229,"post_content":"<p>anything else</p>\n\"some quoted text\"\n</body>\n</html>"},{"post_change_time":1545497572895,"post_content":"</body>\n</html>"},{"post_change_time":1545497512186,"post_content":"<p>Good morning.  We continue with R level 2 - now we are learning about xgboost.</p>\n\n<p><strong>str() function in R</strong> - at least where tibbles or dataframes are concerned - is the <i>structure</i> function.  Contrast that with str() in Python, which is for converting things to <i>strings</i>.\n\n<p>Oh, ok, they threw this in on the side in the xgboost tutorial: we have a select_if function for dataframes, like select but we can do things like: </p> \n\n<pre>data_frame.select_if(is_numeric)</pre>\n\n</html>"},{"post_change_time":1545481514184,"post_content":"<p>That's a little better, I feel like I can see what I am writing.</p>\n\n<p>I have it on the to-do to use the ACE editor here.</p>\n\n<p>Ok, back to the R tutorial.</p>\n\n<p>Interesting, the R tutorial says Kaggle has everything in CRAN installed. </p>\n\n<p>Thus far level 2 R tutorial and the Siraj material, I think are the only tracks covering more advanced topics in machine learning.</p>\n\n<p>This will probably come in handy some time.  Here are lists of stop words in 19 different languages: <a href='https://www.kaggle.com/rtatman/stopword-lists-for-19-languages'>https://www.kaggle.com/rtatman/stopword-lists-for-19-languages</a>\n\n<p>Interesting technique, Tatman removes very common words hotel and room from hotel reviews for applying LDA.</p>\n\n<p>There are more links to text analysis topics at the end of the latent Dirichlet analysis and term frequency / inverse document frequency section of the R tutorials.</p>\n\n<p>Ok, onto the ggplot section...</p>\n\n "},{"post_change_time":1545425554522,"post_content":"<p>Back to the R tutorial</p>\n\n<p>Something interesting to keep in mind when working with these Jupyter notebooks with the R kernel: when I run a cell, and scroll down to some part of it (the cell has a lot of output), and then re-run the cell, the browser (FireFox) automatically scrolls to where I was, so this could be a source of confusion.</p>\n\n<p>I think I'm going to make this text area larger, it's really small right now.</p>"},{"post_change_time":1545424168000,"post_content":"<p>Entry for December 20th, 2018</p> <p>Starting off the daywith some more work on the blog software since we are starting early.</p> <p>Ok,been coding up the save blog post functionality for a while yet this morning,just found another really nice vim shortcut that I'm sure everyone alreadyknows, but I am writing about it to help it stick in my memory, and thisshortcut is hitting '*' in command mode, will jump to the next occurrence of aword under a string, almost as good as 'goto definition' in and IDE.</p> <p>Back to the Kaggle SQL tutorial...</p> <p> Something strange - it seems I amable to query the open air quality database in the forked notebook, but not inanother one that I created from a different exercise. There I am getting a 403,forbidden error, here not.</p> <p> So here's something, looks like BigTablesupports temp tables, which is something I do not think Hive does. For thatmatter, these BigTable queries seem to go a lot faster than Hive queries I'mused to, but that may also be because these tables are really not that big.</p><p>Just finished the SQL tutorial. There is not much there for anyone who hasalready been writing a lot of queries. For me, because I have not used an SQLflavor that allows these common table expressions (CTE's also known as temporarytables), this is good to know about.</p> <p> Predicting time series data is anice intro to long term/short term memories LSTM's that I do not understand.</p> <p>The other stuff on time series is a lot of code, and not a lot of commentary. The videos are well done, but I am still getting used to learning from videos.</p> <p> On to the R module. I see this one has two levels.\n<p> Notes for December 15th, 2018</p> <p>For my 5 day plus training, I will be studying the material in </p> <ul> <li><a href='https://www.kaggle.com/learn/overview'>https://www.kaggle.com/learn/overview</a></li> </ul> <p> If I have time I will also study:</p> <ul> <li><a href='http://karpathy.github.io/neuralnets/'>http://karpathy.github.io/neuralnets/</a></li> <li><a href='https://github.com/GokuMohandas/practicalAI'>https://github.com/GokuMohandas/practicalAI</a></li> </ul> <p>Useful things I learned from the Python intro:</p> <ul> <li>First lesson: <ul> <li>Python floor operator: //</li> </ul> </li> <li>Lesson on functions &amp; getting help: <ul> <li>can give int() function as key to min, <i>i.e.<i/>: min(string_1, string_2, string_3, key=int) </li> </ul> <li>Lesson on booleans: <ul> <li>Python will automatically convert booleans to integers: True + True = 2.</li> </ul> </ul>\n<p>put some html markup in there</p> <h1>Post it</h1>\ntest 2258\ntest 1\n<p>Entry for December 17th, 2018</p> <p>Starting off the day with Kaggle questions.</p> <p>The first thing I notice is that the commit utility is rather slow - so it seems like it always takes a few minutes to save one's work when using the Kaggle exercise system.</p> <p>Today I would like to continue doing the Kaggle exercises, as well as some of the items in the to-do list.</p> <p>The project board is a really great tool in this Wiki for planning out work.</p> <p>Here is a good reference for desigining objects in JavaScript: <a href='https://crockford.com/javascript/private.html'>https://crockford.com/javascript/private.html</a> </p>Wow! really nice, just paste the commit SHA into a note in a Github project note on the kanban board, and it becomes a hyperlink!</p> <p>Ok, done adding some object orientation to the blog software</p> <p>In the slot machine demo - I didn't think to subtract the cost of playing the slot machine itself. Also, interesting opportunity to apply the law of large numbers. The experiment is computing the average cost of playing the slot machine, and repeating it multiple times and averaging the outcome of the experiment will give a value that approaches the true expected average cost.</p> <p>Something to note, that one might get lazy about with Python is we don't have to worry about String.equals, I can compare strings with str1==str2</p> <p>Interesting construct I never heard of: dictionary comprehensions, syntax is similar to list comprehensions, but using {}'s instead.</p> <p>Well, I feel a little small about this, but, at least for the Pandas data frame that the describe() function returns, the syntax to access an element of that description data frame - say the data frame is named df - df['column_name']['row_name']. I am used to thinking of 2-dimensional arrays as accessed by [row][column], so that's the transpose of what I am used to.</p> <p>The machine learning tutorial is very basic, best for someone who knows zero about machine learning.</p>\ntest 2255\ntest 2303\ntest text\nas they say on slashdot, first post\n<p>Entry for December 19th, 2018</p> <p>Now, it seems we get to the good stuff - things I've never really heard about before. Transfer learning. Take a trained model, replace the last layer with something new, and train just that last layer. This is extremely useful.</p> <p>Lesson 4 of the deep learning tutorial covers this.</p> <p>Interesting, it seems the example code is importing from Keras libraries within TensorFlow, not Keras by itself.</p> <p>Interesting note from lesson 6 in the Deep Learning track: Becker says, models with dense layers for tabular data, models with convolutional layers for images. Why?</p> <p>Batch size = # of rows (tabular data), or # of images (image data)</p> <p>'...use validation scores as ultimate measure of model quality...' - Dan Becker, Kaggle Deep Learning Track, Lesson 7</p> <p>Really important side note: in vim, in whatever the non-insert mode is called, I think it is called command mode, but if you have the cursor on the curly brace or whatever and you type '%' it will jump to the closing, and oh, wow, ctrl-o goes to the previous location, and ctrl-i or &gt;tab&lt; goes to next location. Those are huge.</p> <p> Ok, on to the data visualization module. The deep learning module is a bare introduction. </p> <p> The data visualization model is, I think best as a reference to see what commands create which types of charts. </p> <p> As an aside, how to extract certain columns:</p> <pre> df=pd.DataFrame({'col1':[1,3,4,5], 'col2':[1,2,3,4], 'col3':[1,4,5,6]}) df.loc[:,['col1','col3']] </pre> <p> In the data visualization track, in the seaborn section, there is a good explanation of box plots. I did not know, or I have forgotten that half the data is in the box. </p> <p> Ok, so I learned something really good from the visualization tutorial - the plotly library is easy to install and use in python now. For some reason I was thinking one needed to get an API key from plotly to use the library, but that is not the case. I just set up a virtual environment with plotly, pandas, and jupyter, and I can do the same plots that are in the tutorial.</p> <p> The other thing that is cool about plotly, that I didn't even know what they are called is choropleths, which are visualizations like maps with countries colored in according to some statistic.</p> <p> Onto their SQL tutorial, it's using google big query, which is interesting. I think Google is offering an alternative to hadoop and especially hive here, maybe HBase + Phoenix too. I wonder which it is closer to.</p>\n<p>Well, I just learned something cool that I can write htmlon the  items in the Github project kan-ban and that works.</p> <p>I am workingon the blog software again now.</p> <p>We are going back to the idea of savingthe posts as json data.</p><p>Last night I realized it kind of doesn't makesense to search the commits totry and figure out when a post is committed.</p>\ntest 2252\n<p>Entry for December 18th, 2018</p> <p>Today, on the GitBlog side I will put in the post functionality, I think.</p> <p>Another thought is I should probably try twitter bootstrap css</p> <p>Right now, I'll continue on the Pandas tutorial on the Kaggle site.</p> <p>Here is a nice one-liner for reading a dataframe from sqlite3:</p> <pre> music_reviews = pd.read_sql_query('select 1545321590990.html 1545321590990.json 1545322276130.html 1545322276130.json 1545322405355.html 1545322405355.json 1545322465365.html 1545322465365.json 1545364170261.html 1545364170261.json 1545364369876.html 1545364369876.json 1545364559794.html 1545364559794.json 1545364707969.html 1545364707969.json 1545364815617.html 1545364815617.json 1545364992845.html 1545364992845.json 2018_12_15.html 2018_12_15.json 2018_12_16.html 2018_12_16.json 2018_12_17.html 2018_12_17.json 2018_12_18.html 2018_12_18.json 2018_12_19.html 2018_12_20.html 2018_12_21.json jsonify-posts.sh from artists', sqlite3.connect('../input/pitchfork-data/database.sqlite')) </pre> <p>And here is how to set the values of indices:</p> <pre> animals = pd.DataFrame({'Cows': [12, 20], 'Goats': [22, 19]}, index=['Year 1', 'Year 2']) </pre> <p>Select the first row of a dataframe df:</p> <pre>df.iloc[0]</pre> <p>First 10 values of a column:</p> <pre> df.column_name.iloc[:10]</pre> <p>Pick and choose rows:</p> <pre>df.iloc[[1,2,3,5,8]]</pre> <p>Still doing the Pandas tutorial, this is the correct answer for number 7:</p> <pre>df = reviews[['country', 'variety']].iloc[0:100]</pre> <p>Which seems to me to be the opposite of what they just quoted in the documentation</p> <p>Another good point about doing the Kaggle tutorials is that it teaches using the Kaggle environment, which is basically Jupyter, but with bells &amp; whistles that Kaggle has added on.</p> <p>Median value of dataframe df's column_name column:</p> <pre>df.describe().column_name['50%']</pre> <p>Wow, didn't know this would work. Subtract mean value of column with name column_name from every value in column_name, using some dataframe named df:</p> <pre>mean_column_name_values = df.column_name - (df.describe().column_name['mean'])</pre> <p>It seems like:</p> <pre>df.column_name.value_counts()</pre> <p>and</p> <pre>df.groupby('column_name').size()</pre> <p>Get the same data, but it is organized differently. It seems like the point of it in the tutorial is that </p> <pre>df.groupby('column_name').size() creates a series with indices that are values we are counting. </pre> <p>df.groupby(['column_1', 'column_2']) creates a multi-index</p> <p>Important syntax note for dataframes - one can reference a column, and then apply a function to every value in the column, like so:</p> <pre>df.column_name.function()</pre> <p>I think the function must be a method of the Pandas Series object.</p> <p>I was thinking of stealing some example code for implementing commit and push for the blog, but it appears to be a dead end - there is some mature code for using the Github REST API in JavaScript, but I would end up adding thousands of lines of code to get that functionality - lots of dependent libraries, so in the end it will probably be better to follow the tutorial and do the steps the author recommends, but write my own functions, referencing the github API documentation.</p> <p> I am trying kaggle in FireFox. It seems chrome is unstable for this. Also, it chrome flickers when committing, which is really not good.</p> <p> I think Kaggle itself, might be kind of busy, but it looks like the commit dialog box that opens does not cause the page to flicker in FireFox, which is wonderful. </p> <p> Just learned something important - pre-trained models are available on Kaggle. For example, Resnet50.</p> <p> Ok, maybe I got too excited there, looks like they're possibly taking credit for some models built in to TensorFlow. Oh, maybe not, I think maybe they have weights stored for us to use.</p>\ntest 2300\ntest 2248\n<p>December 16th, 2018</p> <p>This morning, I went off on a tangent. Github has a rest API. I added some proof-of-concept code to this page in the js/site_functions.js file. It should be possible to read and write blog posts using the Github rest API. The proof of concept code is now setting the contents of the div with id blog_posts at the top of this page. </p> <p>Back to the Kaggle tutorials, onto a review of lists in Python. <p>Things were going great with the Github API blog project, until I blew my quota. I did not know unauthenticated requets are limited to 60 per hour per IP address.</p> <p>Interesting things from Kaggle tutorial:</p> <ul> <li>Lists tutorial:</li> <ul> <li>Python numbers have a .as_integer_raio() method that returns the numerator and denomenator.</li> </ul> </ul> <p><i>2305</i> I feel like I might have gone overboard with the blogging software today &semi; I meant to work on this slowly, over a longer period of time, but it is quite addictive.</p> <p>So, anyway, now we can write blog posts and retrieve them, but there's no limit on the number of posts we might show, and we need to add some error message if the user is over the quota</p> <p>Which brings me to the second point, this blog software really does not work for mass consumption, at least at present because after a few refreshes, the account that the github.io project that hosts the blog will get rate limited.</p> <p>What we need is to statically render the page that displays the posts - probably when a new post is added.</p> <p>The page where one can add a post should be oath token protected. <p>We also need to add a user-friendly error message when the account is over quota.</p>"},{"post_change_time":1545423233898,"post_content":"<p>Level 2 of the R tutorial starts off with a nice link: to an online book: <a href='https://r4ds.had.co.nz/'>R for Data Science</a> \n\n<p>I think that before I go any further, I will concatenate all the old posts and put them in one new post</p>"},{"post_change_time":1545422947953,"post_content":"<p> Ok, finallly getting to the R tutorial on Kaggle.</p>\n\n<p>Ok,  so R has these tibble objects that are like tables.  I wish it were called something else.</p>\n\n<p>That is interesting,when I start the R practice notebook, I get a disclaimer dialog box about competitions.</p>\n\n<p>Some of the material in the R machine learning tutorial seems to be almost a word-for-word copy of the material in the Python machine learning tutorial.</p>\n\n<p>Ok, on to level 2 of the R tutorial.  It was mostly a repeat of level 1.</p>\n\n<p>I am going to save this blog post and see what happens.</p>"},{"post_change_time":1545414308478,"post_content":"<p>what happens if I use html</p>\n<ul><li>test</li></ul>"},{"post_change_time":1545414242785,"post_content":"Well, so much for working on the Kaggle tutorials this morning.\n\nI spent most of my time on this blog software, and now it is pretty much where I want it.\n\nThe only thing, I suppose is maybe put in some option to show older messages.\n\nAlso, I notice it doesn't pull newer messages like I would like. But it's a start, I'll start touting it."},{"post_change_time":1545413753679,"post_content":"Test I love Sabrina 1235"},{"post_change_time":1545413502185,"post_content":"test from google chrome"},{"post_change_time":1545412751945,"post_content":"test 1219 $$$"},{"post_change_time":1545412703025,"post_content":"test 1218"},{"post_change_time":1545412641529,"post_content":"test 1217"},{"post_change_time":1545412040852,"post_content":"test 1207"},{"post_change_time":1545411813198,"post_content":"test 1203"},{"post_change_time":1545411143027,"post_content":"test 1152"},{"post_change_time":1545410863295,"post_content":"test post 1147"},[],{"post_change_time":1546049598229,"post_content":"<p>Ok, so we've done a little work here</p>\n\n<p>There is now a...I think we call it a carousel thing at the bottom that one can use to browse older posts. I am still at a bit of a loss for a user interface for older posts.</p>\n\n<p>I wonder if I should add a feature for storing encrypted data.  I think enough of that would irk the powers that be here, though.</p>\n\n<p>Before I forget, there  is a really important keyboard shortcut I want to remember for Emacs ansi-term, and that is &lt;ctrl-c&gt;-c &lt;ctrl&gt;-j, which allows one to move the cursor around the window when one has Emacs in ansi-term mode.</p>"},{"post_change_time":1546055933972,"post_content":"<p>OK, I keep getting side-tracked with Emacs.  Somehow I am drawn to it, like a moth to a flame.  I have been ever since I found out about it, some time ago.  I remember hearing people say something like, \"Don't use that Emacs, it's too complicated...\"</p>\n\n<p>Interesting, I immediately decide it is time to rebuild TensorFlow from source, just as soon as I start working on the exercises in the statistics book: <a href='http://www.stat.wmich.edu/s160/hcopy/book.pdf'>http://www.stat.wmich.edu/s160/hcopy/book.pdf</a>.  OK, I'll do some exercises first.</p>\n\n<p>We've just discovered the wonderful world of stemgraphic since the authors mention that in the statistics book, and we are writing iPython notebooks to do the exercises.</p>\n\n<p>Looking for permalinks in StackOverflow - there is a subtle, \"share,\" link at the end of every answer, that has the permalink.  I had to use &lt;ctrl&gt;-f in my broswer to find it.</p>\n\n<p>I am taking a lot of time to formally write up the answers to the questions in this statistics book, and I am wondering if it is worth the time.  At least, we've learned to generate a stem-leaf plot.  Here is a link to the in-progress Jupyter notebook:<a href='https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions/blob/ch1/ch1/exercise-1.3.1.ipynb'>https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions/blob/ch1/ch1/exercise-1.3.1.ipynb</a>. Wow, I didn't know or I had forgotten that Github renders notebooks faithfully as they look in Jupyter.</p>\n\n<p>I just realized, probably the most important feature to add now is one that enables editing existing posts.  At the moment, this is not directly possible.  I am behind on documenting any functionality.  That is for sure.</p> "}]