[{"post_change_time":1585505197920,"post_content":"<p>Got the experiments duplicated, I guess, running them now to see what happens.  So now we are working on the information retrieval project now.\n<p>Need to watch the lecture, but we are on to watching other presentations in the lectures, so I am focused on creating my own.\n<p>Trying to remember something I wanted to do for the class imbalance problem.  It's gone now, but I swear it was something I thought was important."},{"post_change_time":1585365958992,"post_content":"<p>After a couple hours of \"The Man in the High Castle\" we get back to studying.  My advisor wants us to look for a couple of papers in the past two years to get the feature set.  So, we do that.\n<p>I've taken to writing notes in Github markdown, that seems to work fairly well, but the school library also has this Folders functionality.  But I don't love it because it does not let me put notes with references.  And like it's poorly engineered and requires too many clicks.\n<p>Trying tinyurl to shorten the links the library database gives me.  They're way too long. Tinurl seems to work great.\n<p>Also, I realized I forget that one can pres enter in Emacs in a bibtex file and that will also open the URL.\n<p>On other things, I feel I've become complacent at work.  Actually, I feel as though I were complacent today.  Still getting the hang of working from home.  It does put a nice filter on things, dealing with people."},{"post_change_time":1585360685521,"post_content":"<p>We found out we had to load the openssl libraries in SLURM before we could install tidyverse in R."},{"post_change_time":1585182616156,"post_content":"<p>Back to working on those experiments.  Hope I'm not under-delivering on writing."},{"post_change_time":1585102697386,"post_content":"<p>Stuck at home like everyone else, I suppose.  I need to stick to the interesting research.   I have a choice as to what is interesting.  Feeling a the pinch at the end of the semester, like usual."},{"post_change_time":1584752460423,"post_content":"<p>Working on experiments mostly today.  For the data we're working with, it seems that tf-idf document represntation gets better weighted average precision and recall, but word count document representations get better false positive rates.  I wonder what the theoretical justification is.\n<p>I think I really need glasses. The print on this screen keeps getting smaller and smaller."},{"post_change_time":1584225017016,"post_content":"<p>Working on another paper and some more experimenting tonight.  All roads poiint to doing a correlation matrix."},{"post_change_time":1583888717687,"post_content":"<p>Got too carried away writing up Weka last night."},{"post_change_time":1583802321949,"post_content":"<p>I guess one cannot have a python script of one's own named \"csv.py.\""},{"post_change_time":1583386756703,"post_content":"<p>Experimenting with catboost, very addictive.  I found that the University computer does have emacs, but we have to load a slurm module to use it.  Need to learn how to put data into the native CatBoost format, that will speed up processing some.</p>\n<p>Also, to use a config file, or not... command line for script looks really long, I remember passing a list as an argument is kind of messy, I guess it could be quoted.</p>"},{"post_change_time":1583029320374,"post_content":"<p>Found out how to do block indent in Emacs: M-x string-insert-rectangle &lt;RET&gt; string &lt;RET&gt;"},{"post_change_time":1582481939216,"post_content":"<p>recompiled python 3.7.6 with some shared option, don't know if that is the root cause, but had to set LD_LIBRARY_PATH then sudo ldconfig in order to fix it.  I finally added that to the virtual environment activate script.  Also not sure if that is the best way to fix this."},{"post_change_time":1582163427483,"post_content":"<p>Still looking into that CatBoost.  There are not a lot of peer-reviewed papers involving CatBoost, yet.  So we're looking more towards kaggle."},{"post_change_time":1582090522607,"post_content":"<p>Compiled Python 3.7.6 with optimizations today.  Python version was getting kind of old."},{"post_change_time":1581873822286,"post_content":"<p>Well, my advisor put me on  CatBoost, so I am looking into that now.  The introductory tutorial is a good video.\n<p>"},{"post_change_time":1581302883783,"post_content":"<p>So, seems like we needed to switch to Java for some of the logic for duplicating Scherpf <i>et al.</i>, but it's a lot of code for that.\n\n<p>Set up Gradle project to use PostgreSQL, but shied away from Spring Boot, probably one reason this thing is so code heavy.\n\n<p>Updated project to use Java 13, since I think we could use a function parameter."},{"post_change_time":1581170564813,"post_content":"<p>So inspired, don't want to do anything but work with this MIMIC-III data, but alas, must do family & social things on Saturday."},{"post_change_time":1581123230231,"post_content":"<p>Woo hoo.  Always helps to know the right word for things.  I figured out a little more about emacs packages, installed hide-region plugin (from 2005 or so).  Then after going through all that, I remembered when I found out about folding code (late in life) - it was that.  Nick said, \"It doesn't matter, so you fold it.\"  So then I find a more recent emacs fold-this package, that I can install with the package manager.</p>\n<p>Well, we get to start on our first experiments for the professor, finally, and ... the computing resources at the school seem pretty nice.  I don't know why our professor isn't requiring everyone to use them, or at least making the option known to other students.</p>"},{"post_change_time":1580964468557,"post_content":"<p>My it's been a long time.  Almost forgot this blog was here.\n<p>Back chasing emacs hide show, finally found the right page agian: <a href=https://www.emacswiki.org/emacs/HideShow>https://www.emacswiki.org/emacs/HideShow</a>"},{"post_change_time":1578879721890,"post_content":"<p>O.K., we are back.\n<p>For several days we did not do any research.  Warming back up with the Kaggle version of the Boston housing data. Apparently adding dropout layers and an extra layer from the embedding does a lot for reducing MAE... at least initially, then it seems to plateau."},{"post_change_time":1578459112546,"post_content":"<p>Working on the housing data, I think we need an embedding, but only for one-hot encoded data.\n<p>Otherwise, I see I still need to better understand the probabilistic nature of machine learning models.  For some reason Goodfellow's GAN definition eludes me."},{"post_change_time":1578258539319,"post_content":"<p>Doing some review with the Kaggle house price estimating dataset.  Starting with the code from here: <a href='https://www.kaggle.com/shanekonaung/boston-housing-price-dataset-with-keras'>https://www.kaggle.com/shanekonaung/boston-housing-price-dataset-with-keras</a>, but using the data from the Kaggle warm-up contest here: <a href='https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data'>https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a>.</p>\n<p>Also, still cataloging uses of the MIMIC-III database.  Late at night, we seem only to be able to muster the strength to do about 10 of them. \n<p>However, somehow we still have the wherewithal to do some kaggling. And watch Messiah on Netflix for two-and-change episodes."},{"post_change_time":1576859280584,"post_content":"<p>Another day, we continue with running the Keras examples, and coding this Android app for Qi's English lessons.\n<p>Well, we have added six classes to the project to use Room with the Phone's database.  It does not help the application be less code-heavy to use this object relational mapping.\n<p>Wow, ok, incorporated database functionality into the flash cards app.  That took a lot longer than I anticipated.\n<p>Running the Keras sequence to sequence character prediction example now.  We had to edit the code slightly, it is expecting the input file to be two columns, tab separated, but there are a lot more columns.  We changed the code to split the lines on tab and take the first two elements of the split array.\n<p>Here is a tip I remembered when running the stateful vs. stateless Keras example.  When we have code that uses matplotlib to draw a plot, and we would rather save the plot to a file, we must use <pre>fig = plt.figure()</pre> before we use any statements that create the plot, and at the end of the code that creates the plot, we must call <pre>fig.savefig()<\\pre>."},{"post_change_time":1576764256083,"post_content":"<p>Well, learning the commands to start and stop instances in Google Compute Cloud from the command line is not as hard as we thought it would be.  We had to do some setup...<pre>\nsudo apt-get install gcloud\n</pre> and then some account configuration, but the config is easy, and the gcloud command tells you what to do.  After that it is pretty straightforward to Google the commands.\n<p>For starting/stopping an instance, the command is <pre>\ngcloud start instance instance-id\n</pre> or\n<pre>gcloud start instance <i>instance-id</i></pre>\n<p>OK, the Keras bidirectional LSTM sentiment analysis example says, \"try using different optimizers and different optimizer configs,\" so we do that.  Optimizers we are trying are: <pre>'adam', 'sgd', 'rmsprop', and 'adagrad'</pre>.  <pre>'sgd'</pre> is not a good optimizer for the task.  I wonder why.  What is the mathematical reason for that?\n<p>Wow, but the 1-D CNN seems to do even better?\n<p>Interesting with the CNN LSTM sentiment analysis example, if we change the LSTM to a Bidirectional LSTM the accuracy appears to improve some."},{"post_change_time":1576719276891,"post_content":"<p>Starting on an app for my son.  I had forgotten, developer options are under system.\n<p>Well, it turned out to be a little tricky to get the phone to connect.  First of all we had to install SDK platform 28.  Android studio does not show this error, but when we do <pre>\n./gradlew clean build\n</pre> from the command line, we saw this error:</pre>\nFailed to install the following Android SDK packages as some licences have not been accepted.\n     platforms;android-28 Android SDK Platform 28\n<pre> Some googling shows us  how to open the SDK manager and install the SDK platform and accept the license.  But after that we do not see Android Studio detecting our phone.  Where it should show at the top of the screen, we see, \"unknown device.\"  Some more googling tells us to change the option in \"Select USB Configuration\" in the Phone's settings.  We change that to PTP or MTP and everything starts working after that.  Here is the Stack Overflow that gives us the hint about changing the USB configuration: <a href='https://stackoverflow.com/questions/49216585/no-permissions-user-in-plugdev-group-are-your-udev-rules-wrong'>https://stackoverflow.com/questions/49216585/no-permissions-user-in-plugdev-group-are-your-udev-rules-wrong</a>.\n<p>We just want a simple app here.  It shows a picture, you tap it, it shows the word that goes with the picture.  Tap it again, it shows a new picture.\n<p>After the learning curve with saving tmux sessions, we were hesitant to do that for Emacs.  However, it seems relatively simple there, just use <pre>M-x desktop-save</pre>.  We will see how that works out later."},{"post_change_time":1576592976837,"post_content":"<p>Finally we have some time off.  Just deciding what do to.  \n<p>How about cleaning out the inbox.  That was nice.  Just deleted about 10,000 e-mails from before 2017.  It's quite refreshing, actually.  Seems like the right way to search for the e-mail is use<p>\n<pre>\nbefore: dd/mm/yyyy\n</pre>\n<p>Wow, <pre>prefix s</pre> in Tmux is cool, it's like it gives an overview of all the windows.  That would be the find for the day, I suppose.  Then press <pre>q</pre> to get out. \n<p>I think we finally figured out what we are doing wrong with Tmux plug-ins.  I had the file name wrong.  I was using <pre>~/.tmux/conf</pre>.  It should have been <pre>~/tmux.conf</pre>."},{"post_change_time":1576379035971,"post_content":"<p>Well, the IRS has a withholding estimator.  I should have known about that.\n\n<p>OK, finally got tensorboard working outside of docker.  Apparently the --bind_all is what we needed.  Probably opens a hole in security now.  The current documentation seems to encourage using Tensorboard in Jupyter.\n\n<p>On the other hand, it appears as though when we have training going with the babi Keras demo and we try to view Tensorboard, our ssh session to AWS freezes up.\n\n<p>Seems like after that the AWS instance crashes.  We are not too interested in the root cause of the crash at this time.\n\n<p>We have not had a chance to do much research today.  We do not want to write that we spent too much time with our family, but we spent too much time with our family.\n\n<p>Anyway, after some clicking around, we found out how to download a big batch of references using Web of Science.  That is a good resource.  I will probably use it in my next paper.  My advisor wants some kind of survey on MIMIC III so I am starting on that now.  Of course, he fired off about 300 questions for me that I wrote down on a piece of paper that now we cannot find.  \n\n<p>We find the Keras examples do not predict on inputs or produce some tangible result for the user.  Perhaps we can look at adding that in.\n\n<p>This is current and useful; we should be looking at ways to combine these API's with those available from Microsoft, Google, and AWS.   What could these do if we put them together?  How can we put them together?  It is a daunting task, though."},{"post_change_time":1576331735946,"post_content":"<p>Still working on that framework. \n<p>I forgot to commit from AWS last night, will do that, then switch to Google.\n<p>Should have known; it is the number of epochs passed to fit that controls the number of metrics in the history."},{"post_change_time":1576295420966,"post_content":"<p>Banished to my office for the night.\n<p>Watched some of Britannia and then Thursday night football with my son. Now, I suppose it is time to look at Tensorboard and the failing images thing.  For some reason I feel like it is important to develop a good framework for exploring neural networks."},{"post_change_time":1576207070771,"post_content":"<p>Just researching the MIMIC-III dataset some today.  "},{"post_change_time":1576033817247,"post_content":"<p>Well, I finally did a little more revising to the entity embedding paper.  Still the last main section I think needs a lot of work.</p> \n<p>We're off to more adventures with recurrent neural networks.  I think I need to read up on those before I plunge into reinforcement learning, but something nags me telling me to focus on that.  The biggest breakthroughs in AI that I know of are in that area - that would be Alpha Go. Has anything major happened after that?  I don't know, not that I have heard of.</p>\n<p>Still need to learn <i>anything</i> about code folding in Emacs.  We are sure it is a common task.  We just cannot seem to find the right terms to search for to find the right documentation.</p>\n<p>Ok, looks like I found it here: <a>https://www.emacswiki.org/emacs/HideShow</a> but the shortcuts are wily.\n<p>It seems as though one must use different hiding/showing techniques for code versus LaTeX.  Emacs appears to be attempting to find matching curly braces in the typesetting script."},{"post_change_time":1576033402291,"post_content":"<p>Oh, I fear that without a deadline looming, I slip into complacency.\n<p>Earlier, I was trying to think of a good survey paper to start.  Maybe one on Keras, but we do not usually see papers focusing on specific technologies. But maybe on RNN's or maybe one on each subject of the Keras examples.  Interestingly, I do not recall a reinforcement learning example in the Keras demonstrations.  I wonder why?  Perhaps Challot does not know reinforcement learning very well.\n<p>"},{"post_change_time":1575835662383,"post_content":"<p>I tried a few other things with that example code, but no luck, so I decide why not to do the keras tutorial, since I don't know enough to debug anyway.  I have a little time to study anything I want, it's easy to get stuck wondering what to pick.</p>\n<p>OK, I know I'm still an Emacs newb but M-x open-menu-bar is pretty cool from the terminal.  I just wish I knew how to make Emacs in terminal-mode in a Mac use option as the meta key.  It works in GUI mode, but not terminal.</p>\n<p>OK... so in Keras, activation is considered a layer...? </p>\n<p>Looking at adding Tensorboard to this examples.\n<p>Just got a reminder, the trailing slash is important on the tensorboard url.  Otherwise I get \"connection reset\" errors."},{"post_change_time":1575725851827,"post_content":"<p>Well we began immediately with technical activities today.  Still looking at this article \n<a href=\"https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398\">\nhttps://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398</a>,\nbut the author didn't add imports which may make it too difficult to run.\n<p>Before I forget again, the image processing algorithm I was interested in is called pix2pix.\n<p>Well, before jumping back to pix2pix, I'm trying Chollet's example which I guess is the source for this style transfer example I've been chasing. But so far, I run into this error, even after rolling back to an old version of Tensorflow and Keras:\n<pre>  File \"/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\", line 199, in fmin_l_bfgs_b\n    **opts)\n  File \"/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\", line 335, in _minimize_lbfgsb\n    f, g = func_and_grad(x)\n  File \"/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\", line 285, in func_and_grad\n    f = fun(x, *args)\n  File \"/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py\", line 327, in function_wrapper\n    return function(*(wrapper_args + args))\n  File \"8.3_style_transfer.py\", line 287, in loss\n    outs = fetch_loss_and_grads([x])\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\", line 3473, in __call__\n    self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\", line 3403, in _make_callable\n    callable_opts.fetch.append(x.name)\nAttributeError: 'NoneType' object has no attribute 'name'\n</pre>\n<p>So I'm a little over this example at this time.  We'll move on to something different.  This example is a couple of year old by now, so maybe there's something more current."},{"post_change_time":1575725394189,"post_content":"<p>Back from the annual Christmas party.  A lot of those going on in the next few weeks.  Fun to be a part of for a little while.  I wanted to get more technical tonight, but we're kind of running out of time."},{"post_change_time":1575603103954,"post_content":"<p>Got to practice writing something every now and again.  I took last night off because we were finished with our data science homework.\n<p>Still, working on the last section of my survey paper. That could use some  tweaking."},{"post_change_time":1575058680093,"post_content":"<p>Just got my hands on the MIMIC-III data.  Hopefully we can do some good research with that.</p>\n<p>Otherwise, just working on this data science homework.  Procrastinating going to the gym.  When homework gets a  little old, I'll procrastinate on that by going to the gym.\n\n<p>Back from the gym, still working on that data science homework.  I love the data science class.  I get so frustrated sometimes.  It's odd.  I work in the field, but I don't get to work on what <i>I</i> want to work on, in the field.  This is not one of those times.  I am actually working on exactly what I find so interesting about data science.\n\n<p>I love Emacs.  I wonder if using org mode would be better than jupyter in a browser.  I find myself scrolling around a lot."},{"post_change_time":1573784652940,"post_content":"<p>Finally got to the Gym one day during the week."},{"post_change_time":1573784620577,"post_content":"<p>Another night of working on Data Science homework.  No work work though.  I should do some writing, though.  Seems like technical writing, and exercising (which I have not done), sort of go hand in hand.</p>"},{"post_change_time":1573698063041,"post_content":"<p>Just noticed I haven't blogged in some time.  Up doing data science and a deployment for work.  "},{"post_change_time":1572750362105,"post_content":"<p>Well, I just noticed I made an expensive mistake; I did not shut down the AWS gpu instance last night.   That'll cost ya.</p>\n\n<p>Checking out the GEM graph embedding package tonight.</p>\n\n"},{"post_change_time":1572572581407,"post_content":"<p>Well, my conscience and my advisor tell me to work some more on my paper while we're waiting on the review to come back.</p>\n\n<p>So I'm going to try that bibliometric software, at least.  "},{"post_change_time":1572489013169,"post_content":"<p>Just trying out GEL. Looks like it needs R-studio.\n<p>I find an urge to write now.  I guess school is paying off that way.  Oh, I've been lazy these past few days.  I was sleepy, now I'm not.  It is hard to switch back to a morning person.\n<p>One thing I've got to check out, this app on github looks like it may be good: <a href='https://github.com/fabian-beck/survis.git'>survis</a> that is this clone url.</p>  Survis looks like a good thing, maybe I can use it to create some of the fancy graphics I see in other papers, especially the survey papers.</p>\n<p>The key thing to search on is \"bibliometric software.\""},{"post_change_time":1572404478214,"post_content":"<p>Just one of those nights, very irritable for some reason.  Maybe some programming will help me cool down."},{"post_change_time":1572228652427,"post_content":"<p>Good evening.  Well the revision is back to the advisor, and the data science homework is mostly done.  The reviewer who is going to want a bunch of changes to my paper is studying for the qualifying exam, so I suppose that catches me a little bit of a break.\n\n<p>One thing I want to do is get a little more hands-on with Guo and Berkhahn's code that got them third place in the Kaggle competition.  So that will be my goal for tonight - to run that in AWS. I guess I should have an Azure account as well, but I don't know how much it's worth it to learn all those clouds.  I guess they all have things to offer that might have a synergy.  There's probably already some people building a business on that now.</p>\n\n<p>I see I may have been a bit snobby about the free GPU resources on Colab.  Really easy to use, just ran the BiLSTM example using GPU support, no issues.  But then I don't get exposure to everything in the clouds.</p>"},{"post_change_time":1572114168657,"post_content":"<p>Wow.  When I don't write in the morning, I lose a lot of time to work on writing.</p>\n<p>Still working on shortening up my table.  I had some things in my LaTeX preamble to indent every paragraph.  That was forcing the indentation in my longtable. </p>\n<p>I installed grammarly last night, seems to have disabled spell check.  I just turned it off again now.</p>\n<p>Well, if I get nothing else out of my data science class, I didn't know there was a paper about Sklearn API design. It is here: <a href='https://arxiv.org/pdf/1309.0238https://arxiv.org/pdf/1309.0238'>https://arxiv.org/pdf/1309.0238</a>. \n<p>Also I noticed it seems like flyspell isn't checking words I misspell automatically.  It's not a complete fix, but if I do M-x flyspell-buffer, I see my red lines come back.\n<p>OK, the revisions were pretty easy to work through, now just... I don't know why I lack confidence with this null hypothesis significance testing.  I guess the MIT course went too fast... now I am remembering the professor said he recorded another video.  Probably it will help to watch it.  \n<p>Anyway, I'm trying to get into the habit of doing a little machine learning experimentation daily, to build some creative wherewithal.  For some reason I am drawn to BiLSTM so I'm starting with that.  I know it is used in some of the papers I surveyed. </p>  "},{"post_change_time":1572054100033,"post_content":"<p>Been a long time.  We got that draft of the paper finished, so, working on another draft now.</p>\n<p>Before I finish this one, I swear I will run it through word.  The hard part right now is editing this table.  It was a mistake to start the table in LaTeX.  The right way is to put it in a spreadsheet.  Then it's far easier to manipulate columns.  The only thing is I think it might be a little more difficult to deal with references.</p>"},{"post_change_time":1571510944471,"post_content":"<p>We finished a long list of references to check for inclusion in the survey.  There is probably a better way to work than what I am doing. I think I am being stubborn.  Just as I started writing this, I realized I have another list of 30 papers to check.  It is important, though.  I feel I must do it."},{"post_change_time":1570246692525,"post_content":"<p>End of a long day.\n<p>Now I remember I forgot to start any blog at work.\n<p>Just started checking out this betago project.  It is really nice work.  I think alpha go is currently the pinnacle of achievement in artificial intelligence, and further advancements lie in that direction.  If anything, I have a hunch that using reinforcement learning to build machine learning models will pay dividends."},{"post_change_time":1569894727577,"post_content":"<p>Good evening.  It has been quite some time since my last post. I have been writing, and this data science class has captured most of my interest.  I feel like I never want to stop taking classes.  I guess some of that is due to having a good professor this semester.\n\n<p>I am liking Emacs and I see that slowly I am able to use it more and more at work.  I could ditch atom entirely if I could just figure out the regular expression syntax a little better. I did some text manipulation today, that I just now realized Intellij might have had some support for."},{"post_change_time":1568941446994,"post_content":"<p>Well, we got our feedback today, so we have some  writing to do.  But first, we do the moral inventory.\n<p>That's done, one reviewer, really easy, not much to fix.  The other wants more work.  Can I get it done by Sunday?  Who knows?"},{"post_change_time":1568941403460,"post_content":"<p>Caught up on work for once, feels kind of weird.  It will not last long.\n\n<p>Don't know if it's wise to have two of these cloud accounts going.  The Google bill doesn't seem to be that bad so far.\n\n<p>Checking out Nvidia NeMo now. Example crashed on me so pausing on that one.  I didn't even know about this GELU activation function, and I think some models are becoming more fashionable, namely transformer models, and attention based models.  We have to get up-to-speed on those."},{"post_change_time":1568569560863,"post_content":"<p>Having a pretty good day today.  Still getting starting writing a little later than I would like.  Having fun writing one-liners for my data science class.\n\n<p>"},{"post_change_time":1568468248413,"post_content":"<p>Well good morning.  Time to  get back to writing.  It is a way of life.\n\n<p>I don't know what my difficulty with Emacs regular expressions is before.  It went pretty  smoothly when I used it just now.  For writing, I think these old style editors with a big collection of keyboard shortcuts is pretty good.  Word's auto-correct is pretty nice until you start using unusual words."},{"post_change_time":1568466288639,"post_content":"<p>Well, I just blew a bunch of time watching a movie, but why not?  It's Friday night.  Nice to spend a little time with family.  OK, back to working on the survey paper.  For some reason I am motivated to do that now."},{"post_change_time":1568422733370,"post_content":"<p>Another day goes by. Deployment tonight, got to work on my data science homework now, do some writing later.  Can't resist this langtool in emacs - I keep making too many grammar mistakes.   So the professor is hinting I need to do something like that.\n\n<p>Woo hoo, sweet.  The language tool is working here on emacs at home.  It's catching all the mistakes that the reviewer found earlier.  We should have done this earlier.  Ah, it's a bittersweet moment.  I just wonder though, without gogole and whatnot, I can't imagine how long it would take to get this all installed.  "},{"post_change_time":1568333731403,"post_content":"<p>Another day, another  post.  Slacking on my personal inventory and  work blog, though.  I have a cold today, so I aim to keep it short tonight.  I just want to to a little writing, then probably go to sleep <i>early</i>. "},{"post_change_time":1568118412036,"post_content":"<p>good morning.  before the clock starts at 9 thought I would start with a post.  Working from home today because we have a sore throat, so have a few extra minutes without the commute.\n\n<p>Wow I really got worked up about running some Java Spark code today.  The key is, in our system anyway, use spark-submit.  If I would have done that my blood pressure would have been lower today.\n\n<p>Now, back to working on that data science homework."},{"post_change_time":1567962624005,"post_content":"<p>Ok, getting ready that e-mail that I can send off with the latest draft of my work.\n<p>Ok... in my googling and what not working on my data science homework, I see IBM is now selling <a href='https://www.ibm.com/cloud/bare-metal-servers?cm_mmc=Display_N1114924.2079317MEDIAMATHPROGRAM-_-Cloud%20and%20Data%20Platform_Cloud%20Platform%20Digital-_-WW_WW-_-253538791_Bare%20Metal%20Lower%20Prices-IT%20Architect-V1-728x90-Nonanimated-Standard%20Load-NULL-NULL-NULL-NULL&cm_mmca1=000016GC&cm_mmca2=10006171&cm_mmca4=253538791&cm_mmca5=120398998&cm_mmca6=AMsySZbR-egOnE93y1UqYdrUJrMo&dclid=CjkKEQjwzdLrBRC5nN_8_cr6g-8BEiQALlhSpglJh2rUubEHzs3OLCDMzdl2ScufgqCadd4OXng2czfw_wcB'>bare metal servers in the cloud</a>\n<p>Ok, that's counter to the intuition I've built up about the cloud."},{"post_change_time":1567862059126,"post_content":"<p>Well, I have a few minutes to work on homework before we leave for the meeting.  It seems there is a little problem compiling png's in tex.  Some error about a bounding box.\n<p>Emacs block select.  One of those I use just not enough to remember yet.  It's ctrl-x, then take your fingers off the ctrl-x, then press space.  Otherwise it does some funky buffer switch thing that I don't understand.\n<p>Back to working on the survey paper.\n<p>Ok, done with revising, now just need to respond to critics.  I decided to leave that for tomorrow.  I want to spend some time on the data science class."},{"post_change_time":1567860507591,"post_content":"<p>Another day another few minutes spent on the paper.  Still multi-tasking and trying to get this training done for work, too.\n<p>We had some fun  with Emacs this afternoon.  I have no idea how to debug lisp, it turns out."},{"post_change_time":1567364219740,"post_content":"<p>Well, I ran out of of steam last night but today we just followed the  instructions for adding proper citations to a Jupyter notebook.  Not to difficult really, just the matter of this .tplx file that is completely new to me, and the syntax seems a bit abstruse what with all the ((* and *)) tags.  Documents I googled on the subject were not quite conveying to me what we need to do to add citations, but this YouTube video: <a href='https://www.youtube.com/watch?v=m3o1KXA1Rjk'>https://www.youtube.com/watch?v=m3o1KXA1Rjk</a> explains everything quite well.</p>\n\n<p>Ok, so we're about to get on with writing but first I will do my moral inventory.</p>\n\n<p>Wow, it must be exercise and somehow less stress even though there is a hurricane coming ... something gives me the ability to focus longer and work on this paper.  Perhaps it is that I have leveled my pride enough after getting the first draft done, that I can face the feedback I received on my first draft.</p>\n\n<p>Now, I would like to take the few minutes I have left here today to study something interesting.  I thought I would take a try at maybe some of the graphics examples we have.  For some reason I'm not terribly interesting in using Jupyter to do much at the moment.\n\n"},{"post_change_time":1567364070376,"post_content":"<p>Good morning.  Awaiting the hurricane, survey paper not done, but table is.  Next survey paper I will start with tabulating some data on a collection of works.  It is a good practice, doing it after-the-fact seems like such a chore.  Also I would like to use one of those fancy graph generators that shows the semantic map of works surveyed.  It's this VOSViewer tool <a href='https://www.vosviewer.com/'>https://www.vosviewer.com/</a> Looks like we can use it to really spice up  our paper.\n<p>Can't resist checking how we can use bibtex in Jupyter, if that's even possible."},{"post_change_time":1566954266635,"post_content":"<p>Working on that survey paper. We got some error in Pandas when trying to run the Bert notebook, maybe I'll take a look at that later.  Doing some Jupyter training for work tonight too.  \n<p>OK, we'll have to try and document the Pandas error and its solution here later.\n<p>"},{"post_change_time":1566954128572,"post_content":"<p>Up  for another late night deployment\n<p>We did some writing tonight, but it still feels like not enough.  That probably means it is not enough.  However, I know I need to experiment and play as well, or it feels like the writing is not worth it.  So tonight, hopefully, we run the bert examples, etc.\n<p>So I like this expensive trick that we could probably do on the cheap with colaboratory:\n<ul>\n  <li>start gpu instance\n  <li>start tensorflow docker container with jupyter\n  <li>ssh -X to gpu instance\n  <li>use firefox to browse notebooks\n</ul>"},{"post_change_time":1566765957146,"post_content":"<p>Another day, another post.  We just got back to the beach.  Before I give myself a haircut, I have time to write a little more while my wife is giving our son his haircut.\n<p>Sigh, it is becoming clear perhaps I need only be using cloud based Jupyter for research - not 100% sure as I do not use them often.  I am seeing two examples, BERT apparently the way to get a demo is with the notebook the authors include on the BERT Github site, and then now I am seeing this nice graph embedding package that is based on a survey paper, <a href='https://github.com/palash1992/GEM'>https://github.com/palash1992/GEM</a> that also has a nice Jupyter demo in the readme."},{"post_change_time":1566702422739,"post_content":"<p>Just dropping a quick note.  It is hard to find time to write - family time takes up a lot of time.  \n<p> Still working on the survey paper.  My reviewers told me to put in a table of works surveyed.  If I had seen that advice before I started I would have done that before I began writing the survey.   That is really good advice.  I can see how my paper would have been far better organized if I had started with writing the table first.  So, it is a lesson learned, and next time I will do that.\n<p>It pays to be intellectually brave but one usually finds out in hindsight. The big discovery today is that all I needed to do to ssh to a Google VM is to copy my public key to authorized_keys on the remote side.  I thought I had done that before and been unsuccessful.  Maybe I was doing something that does not make sense, like using the wrong key."},{"post_change_time":1566442654566,"post_content":"<p>Well, I was actually inspired to start writing right away.  I need to put in a table of works surveyed, so that was easy enough to get started on.  Still, I need to put more time into these things.\n<p>Now, I'm off to look at reinforcement learning. Checking out this (advertisement laden) <a href='https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/'>https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/</a>.  Still, it is a good introduction, I would say, far less daunting than Ng's exposition in Stanford <a href='https://www.youtube.com/playlist?list=PLA89DCFA6ADACE599'>CS 229</a>."},{"post_change_time":1566354462993,"post_content":"<p>Starting off writing for the day, finally.\n<p>Had to do some personal writing as well.  Perhaps it is the case that I need to write about some personal things first before I can move on to technical matters.  For sure, I do not feel inspired lately and I have to muster strength to write.\n<p>I keep at it because I am sure it is the right thing to do.\n<p> Well that was a slog but I got through all the trivial corrections one of the reviewers gave.  Technical writing is not easy, at least, not on this topic, not for me.\n<p>For some reason I am interested in autoencoders tonight, so I think I'll fire up Jupyter hub."},{"post_change_time":1566182168212,"post_content":"<p>Good morning.  Attempting to install Docker Tensorflow that can use GPU's by heart.</p>\n<p>Ok, it is really  not that bad, I hope I do not forget the steps again. It is basically these steps:</p>\n<ul>\n  <li>start a virtual machine with a GPU</li>\n  <li>install nvidia drivers - I guess the cloud providers don't install drivers because their customers all want different versions.\n  <li>install docker</li>\n  <li>install nvidia container toolkit</li>\n  <li>run tensorflow docker container.  Note: it appears to be necessary to use the <pre>gpus all</pre> option when starting the container, otherwise TensorFlow is unable to use the GPU device.\n </ul>\n<p>I will do these steps in AWS now just to see if there is any difference and to kill about a half and hour.</p>\n<p>Just doing some writing here as a warm-up.  I spent a few hours at the beach where I took a jog, then went swimming.  Then we went out for lunch and we did the grocery shopping, so I was feeling a little drowsy.  Now I am drinking a little coffee and having a nicotine lozenge.  I will have to step away for a couple of hours.</p>\n<p>"},{"post_change_time":1566066988177,"post_content":"<p>Ok, blogging about it but I swear I have made notes on this before; I just can't find them now.\n<p>For the umpteenth time I am troubleshooting starting tensorflow with a cloud machine that has a GPU.\n<p>This time we are using a Google Compute Cloud Ubuntu 18 box with a P-100 GPU.  \n<p>Taking snapshots, used:\n\n<p><pre>sudo ubuntu-drivers autoinstall</pre>\n\n<p>To install nvidia driver.  Source for command is: \n<a href='https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-18-04-bionic-beaver-linux'>https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-18-04-bionic-beaver-linux</a>\n\n<p>Then did common docker community edition install from <a href='https://docs.docker.com/install/linux/docker-ce/ubuntu/'>https://docs.docker.com/install/linux/docker-ce/ubuntu/</a>\n\n<p>Then installed nvidia-container toolkit using commands from:\n\n <a href='https://github.com/NVIDIA/nvidia-docker/blob/master/README.md#quickstart'>https://github.com/NVIDIA/nvidia-docker/blob/master/README.md#quickstart</a> \n\nin the section under ubuntu 18.04.\n\n<p>Then minor detail, but add the user to the docker group:<pre>\n sudo usermod -a -G docker $USER\n</pre>\n\n<p>Then we can run a recent tensorflow image:<pre>\n docker run -it --gpus all tensorflow/tensorflow:1.13.2-gpu-py3 bash\n</pre>\n\n<p>Another important note is that we used a Tesla P4 GPU for this work.\n\n<p>No, off to run word2vec"},{"post_change_time":1565914230501,"post_content":"<p>It is more difficult being a writer than I thought.\n<p>I write a little here to get warmed up.\n<p>I did not know that when I signed up for the Ph.D. program, I was signing up to be a writer, or rather, I did not think through what it all meant.\n<p>We shall see if I find anything cool later. \n<p>My wife is helping son with homework now, so I must do some as well.\n<p>Excellent.  Apparently my experimentation with Google Compute Cloud last night cost all of about $0.23.  Super.   I need to find a way to auto-shut-down instances so I don't accidentally run up a bunch of charges."},{"post_change_time":1565836957254,"post_content":"<p>Well submitting the paper for review was a humbling experience.  I see I make more spelling mistakes than I knew, when referring to a section in a technical paper one should spell, \"Section.\" \n<p>Somehow, in the midst of this, I find I must do some experimentation with algorithms or I become totally uninspired.\n<p>So, I first fire up a google compute cloud gpu instance, but have driver problems, which I seem to remember AWS instances where I needed to install updated or the right version of drivers before we could run the tensorflow docker containers.\n<p>Anyway, that was a no-go, so I just used Keras locally and ran the xor example, mostly from here: <a href='https://blog.thoughtram.io/machine-learning/2016/11/02/understanding-XOR-with-keras-and-tensorlow.html'>https://blog.thoughtram.io/machine-learning/2016/11/02/understanding-XOR-with-keras-and-tensorlow.html</a> with a little more plotting from here: <a href='https://chrisalbon.com/deep_learning/keras/visualize_loss_history/'>https://chrisalbon.com/deep_learning/keras/visualize_loss_history/<a>. It's too much work to post the example, it's mostly stolen work anyway."},{"post_change_time":1564766500878,"post_content":"<p>Just sent the draft of my survey paper to my advisor.  Now we wait until it gets sent back with a bunch of criticism pointing out everything I did wrong and more work to fix all of it.\n<p>Except now, I don't know what to work on.\n<p>OK, so I start doing problems from  \n<a href='http://onlinestatbook.com'>http://onlinestatbook.com</a> only using Jupyter do do that.  Sorry for being such a noob, I didn't realize how easy it is to put  LaTeX in Jupyter.  I'll start doing that more. Here is a relevant document on that subject: \n<a href='https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Typesetting%20Equations.html'>https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Typesetting%20Equations.html</a>.\n<p>I guess this is why doing exercises is a good idea.  Probably everyone already knows this but there is a nice Python package  for symbolic computation: <a href='https://docs.sympy.org/'>https://docs.sympy.org/</a>"},{"post_change_time":1564766304847,"post_content":"<p>Well, after taking the weekend off, with feelings of guilt I return to writing."},{"post_change_time":1564231952955,"post_content":"<p>Well, it has been nearly a month since I have blogged.\n<p>We are writing this morning as a warm-up for working on the survey paper, which has dragged on for another month.\n<p>My advisor is starting to get impatient with my lack of progress, so I am 99.5% sure I am going to take next week off.  I just need to clear it with my boss."},{"post_change_time":1561853179378,"post_content":"<p>Still working on the neural network embeding problem.  I think that for sure embedding tabular, categorical sorts of data is vital for neural networks' becoming mainstream.</p>"},{"post_change_time":1561078336632,"post_content":"<p>Well the intro to neural networks is over.  So now I have a little time to experiment.  It's funny how our minds give us permission to explore under the right circumstances.</p>\n<p>Anyway, I finally found out about RefTex. Nice emacs mode.  Another one of those things that makes me feel silly about not contributing back to open source.  There's so many great, amazing tools out there.</p>\n<p>Anyway, after I put emacs in reftex mode (M-x reftex), I can then do C-c [ and start searching references to cite.  Very slick!</p>\n"},{"post_change_time":1558874835680,"post_content":"<p>Using emacs to do technical writing - seems more and more like the best way to go.</p>\n"},{"post_change_time":1558789424585,"post_content":"<p>There is a saying, \"You can't win the lottery if you don't buy a ticket.\"</p>\n\n<p>I think it's smarter to say, \"If you buy enough tickets, you'll win the lottery.\"</p>"},{"post_change_time":1558056369209,"post_content":"<p>Hi blog, long time, no write.</p>\n<p>We finished the qualifying exam, now have a survey paper to write.  Just can't make the time to start it.</p?\n<p>Just started the introduction to neural networks class, but have this big data project for work that I can't put down.</p>\n"},{"post_change_time":1553955507424,"post_content":"<p>I have decided I don't quite like minghe's YOLO example from  Kaggle, I think I would rather try Darknet's example instead.  At the same time, it is important to learn how to deal with Imagenet and Imagenet-sized data.<p>"},{"post_change_time":1553907599201,"post_content":"<p>Well the weekend is here, and I am slowly letting go of the work day.</p>\n<p>Back to trying to get that Kaggle YOLO example to work.</p>"},{"post_change_time":1553857364520,"post_content":"<p>Good morning.</p>\n\n<p>My advisor only gave me 6 papers to read, so I've got my hands full with that.</p>\n\n<p>I did not realize that embedding categorical values for input to a neural network would be so fruitful.  We discussed that at work a long time ago and never revisited the idea.  The next time we discuss possibilities I am going to search for literature on the topic.</p>\n\n<p>We are forming a deep learning group at school.</p>"},{"post_change_time":1553733378711,"post_content":"<p>Ok, reading some papers for school tonight</p>\n<ul>\n  <li>< a href='https://www.fast.ai/2018/04/29/categorical-embeddings/>https://www.fast.ai/2018/04/29/categorical-embeddings/</a></li>\n  <li><a href='https://arxiv.org/abs/1604.06737'>https://arxiv.org/abs/1604.06737</a></li>\n  <li><a href='https://aclweb.org/anthology/Q17-1028'>https://aclweb.org/anthology/Q17-1028</a></li>\n  <li><a href='https://www.aclweb.org/anthology/N16-1162'>https://www.aclweb.org/anthology/N16-1162</a></li>\n  <li><a href='https://openreview.net/forum?id=By03VlJGG>https://openreview.net/forum?id=By03VlJGG</a></li>\n  <li><a href='https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5370-x'>https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5370-x</a></li>\n</ul>\n\n"},{"post_change_time":1553561710503,"post_content":"<p>Good evening.</p>\n\n<p>Just wanted to make a note of a good blog: <a href='https://towardsdatascience.com/deep-dive-into-convolutional-networks-48db75969fdf'>Deep Dive Into Convolutional Neural Networks</a>.  This is a good synopsis of techniques that are out there now. Good reference list as well.</p>\n\n<p>This is a very cool program.  Darknet - seems to have a lot of interesting implementations.  I used the yolo implementation, it seemed to run very well on a GPU: <a href='https://github.com/pjreddie/darknet'>https://github.com/pjreddie/darknet</a>.\n\n"},{"post_change_time":1553040680144,"post_content":"<p>Good evening.</p>\n<p>Watching more of those Winston lectures.</p>\n<p>Knowledge engineering... that is a new term for me today.  I didn't think of it before but Winston is right, watching experts work is informative about how they work because they will do things they do not remember to tell you they do when they do their jobs."},{"post_change_time":1552960988714,"post_content":"<p>Good morning.   I found what might  be a good reading list here <a href='https://ocw.mit.edu/courses/mathematics/18-657-mathematics-of-machine-learning-fall-2015/readings/'>https://ocw.mit.edu/courses/mathematics/18-657-mathematics-of-machine-learning-fall-2015/readings/</a>.  This is a reading list from the MIT course on Mathematics of Machine learning, 18.657 .</p>\n\n<p>Quote from MIT professor Patrick Winston, lecture number 2 from MIT 6.034: <a href='https://youtu.be/PNKj529yY5c?t=2607'>https://youtu.be/PNKj529yY5c?t=2607</a> .<blockquote>\nThings to ask when facing a new problem: Knowledge is power.  \nCatechism: What kind? How embedded? How used? How much? What exactly?\n</blockquote>\n<p>Thanks.</p> \n<p>Then he crosses it out and says it's knowledge about knowledge is where the real power is.</p>\n\n\n"},{"post_change_time":1552866536325,"post_content":"<h1>Importand docker commands for today:</h1>\n<pre>\ndocker build  --tag=maml .\n</pre>\n<p>I was going to get into a huge thing with all these docker commands,  but this is all documented here: <a href='https://docs.docker.com/get-started/part2/'>https://docs.docker.com/get-started/part2/</a>.</p>\n\n<p>Anyway, we have two new repositories today.  One for aws scripts: <a href='https://github.com/jhancock1975/aws-utils'>https://github.com/jhancock1975/aws-utils</a>, and another for docker repositories.  I wonder if this is the right way to do it, but I don't see a place in docker hub for the Dockerfiles, so I started a repository with dockerfile information for a couple of images we built today for running various deep learning code: <a href='https://github.com/jhancock1975/docker'>https://github.com/jhancock1975/docker</a>.</p>"},{"post_change_time":1552831084832,"post_content":"<p>Ok, cool.  After some trial-and-error, tensorflow in nvidia-docker seems to work well.</p>\n\n<p>Here is an interesting benchmark that I am willing to bank on: <a href='https://blog.exxactcorp.com/is-docker-ideal-for-running-tensorflow-lets-measure-performance-with-rtx-2080-ti/'>https://blog.exxactcorp.com/is-docker-ideal-for-running-tensorflow-lets-measure-performance-with-rtx-2080-ti/</a>.  The  TLDR for that is that running tensorflow in docker is only slightly slower than running it natively.  It looks like about 1-2% slower by application specific metric like, \"images per second.\"</p>\n\n<p>At any rate, I needed to realize that I need to use the right tag for the docker container with the version of tensorflow and python that I desire, and this is not the tag they use in the example commands in the current tensorflow documentation.</p>\n\n<p>Yeaz...the nvidia docker image seems to be running dcgan-tensorflow just about as quickly as I remember seeing it run natively on a GPU before.</p>\n\n<p>Just thinking out loud, and I bet someone is already working this way, but the best way to run an experiment in AWS is to fully script everything so the instance or container starts up, runs some code, saves results somewhere, and then shuts itself down.  I would like to start working this way.  I think it would save me money in the long run.</p>\n\n"},{"post_change_time":1552738948134,"post_content":"<p>The qualifying exam is over, so I am back into research.  For some reason I keep thinking about the gradient descent by gradient descent paper.</p>\n\n\n<p>Google compute cloud - having trouble getting around the quota thing for a graphics vm, so sticking with Amazon for now.  I see quota increase requests have a two day SLA which is  a big turn-off.</p>\n\n<p>so the thing is now getting back into using AWS gpu instances.  I see there are ARM instances now, which is extremely interesting.  Maybe I will try running TensorFlow on one of those later.</p>\n\n<p>Ok, ran into this issue where nvidia-smi hangs after installing.  I used the directions here <a href='https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-18-04-bionic-beaver-linux'>https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-18-04-bionic-beaver-linux</a> using the PPA and the <pre>sudo ubuntu-drivers autoinstall</pre> command and that worked pretty well.  Nvidia-smi works at least.</p>"},{"post_change_time":1552243303702,"post_content":"<p>Noticing something about calculating error in machine learning algorithms.\n</p>\n\n<ul>\n<li>perceptron - update using  desired - actual</li>\n<li>single node artificial neuron - update using desired - actual</li>\n<li>multilayer neural network with backpropagation - update using desired - actual </li>\n<li><calculating error for a model based on <b>2<sup>2</sup>r experiment design</b> use desired - actual</li>\n<p>For experimental design the desired is the value of the response for some combination of factors and interactions, and the actual is the model prediction for that experiment.<p>\n\n<p>So in all cases we subtract the model output value from the desired output value.  I think it must be convention.</p>\n\n<p>I seem to have missed the point earlier.  These models that we create from 2<sup>n</sup> experimental designs seem to estimate the mean value of the actual response as the predicted response.</p>\n\n<p>I notice on my TI-30XIIS I must put parenthesis around negative numbers I need to square.</p>\n\n"},{"post_change_time":1552080729932,"post_content":"<p>Spilled water on a laptop keyboard for the second time in about a calendar month.</p>\n\n<p>Everything seems to be working ok, but I thought I'd try and do a little heavy typing to see if things start going wrong.  Actually, it seems spilling the water may have cleaned things up and the keyboard is actually working better than before.</p>\n\n<p>Great resource for digital logic - this one on JK flip flops in sequential circuits: <a href='https://www.youtube.com/watch?v=iXJChs89p5I'>https://www.youtube.com/watch?v=iXJChs89p5I</a></p>"},{"post_change_time":1552052121459,"post_content":"<p>Still studying for the exam, it seems like so far I can only cover 3 or 4 subjects per day.</p>\n\n<p>liking using more emacs as c/c++ ide for data structures exam review</p>\n\n"},{"post_change_time":1551884682846,"post_content":"<p>On PTO to study for the qualifying exam!</p>\n<p>I notice the power of writing things out by hand sometimes to gain an understanding.  I suppose it relates to how the professor that is giving the work learns. I need to try and use the same way of learning things as the creators of these documents.  This probably applies to writing papers as well.</p>\n"},{"post_change_time":1551703736533,"post_content":"<p>Good morning.</p>\n<p>I forgot about copyq - we have some good old papers to research for the qualifying exam. I wonder about using them in my work.  Let me go to the fabulous copyq clipboard and get it.  The link to the course is: <a href='http://www.cse.fau.edu/~taghi/classes/cap6673/'>http://www.cse.fau.edu/~taghi/classes/cap6673/</a>.  Fabulous. </p>\n\n"},{"post_change_time":1551527809058,"post_content":"<p>We need some sort of AI layer or component that cuts across layers.  Aspect oriented programming is a possible candidate, but we need the AI aspect to have a lot of state.</p>"},{"post_change_time":1550350740753,"post_content":"<p>I seem to have found some inspiration.</p>\n<p>I suppose it was writing a letter to my brother who does not have phone access at the moment, that did something for me, psychologically just now.</p>"},{"post_change_time":1550061692061,"post_content":"<p>Still need to get cracking on that qualifying exam studying.  I cleared it with my boss and co-workers that I will take some time off to study before the test, so that is a relief. </p>"},{"post_change_time":1549888160383,"post_content":"<p>Good morning.\n\n<p>I am still really not doing enough to get ready for this exam.  At least it is February 11th, and not February 12th.</p>\n\n<p>OK, onto studying.</p> "},{"post_change_time":1549809284446,"post_content":"<p>Hello blog.</p>\n\n<p>It has been quite some time since my last entry.  I have been wallowing in the anxiety of preparing for this qualifying exam.  Getting started on studying is the most difficult part.  We have about 18+15=33 days left to study.</p>\n"},{"post_change_time":1547988224428,"post_content":"<p>Back to studying, and blogging.</p>\n\n<p>For some reason this week I have been rather un-prolific. I've cut out a \ncouple of indulgences in my personal life, and I believe that robbed me of some motivation.</p>\n\n<p>OK, so anyway,studying for COT 3530 section of the qualifying exam.  COP 3530 is my school's identifier for their undergraduate data structures class.  Unfortunately for me, the text book is about $200, so I'm thinking I will chance it and go off the topics in the syllabus, and find exercises on line in order to study.  In fact, I'm looking forward to that.</p>\n\n<p>Ugh.  My wife has discovered vapid piano music streams on YouTube.  Good thing I'm in relatively good mental health otherwise this music would be depressing.</p>\n\n<p>OK, just shelled out about $45.00 for a used copy of the text book.  Somehow I feel more secure about preparing for these exams having a copy of the text.</p>\n\n\n\n"},{"post_change_time":1547929499139,"post_content":"<p>Back to studying, and blogging.</p>\n\n<p>For some reason this week I have been rather un-prolific. I've cut out a \ncouple of indulgences in my personal life, and I believe that robbed me of some motivation.</p>\n\n<p>OK, so anyway,studying for COT 3530 section of the qualifying exam.  COP 3530 is my school's identifier for their undergraduate data structures class.  Unfortunately for me, the text book is about $200, so I'm thinking I will chance it and go off the topics in the syllabus, and find exercises on line in order to study.  In fact, I'm looking forward to that.</p>\n\n<p>Ugh.  My wife has discovered vapid piano music streams on YouTube.  Good thing I'm in relatively good mental health otherwise this music would be depressing.</p>\n\n"},{"post_change_time":1547386988059,"post_content":"<p>test</p>\n\n<p>OK, had to see if that was really going to save.</p>\n\n<p>Starting on the personal inventory now.  I realize this morning that part of studying for the qualifying exam is going to meet with my professors - before I had it in my mind that studying only involved the sitting on my ass part of getting ready for the exam.  </p>\n\n<p>OK, so I realized something about Emacs ansi-term just now.  For editing long command lines that wrap, the best way to deal with that is to put the terminal in char mode with C-c C-j, then edit the long line, then go back to line mode with C-c C-k.</p>\n\n"},{"post_change_time":1547386888133,"post_content":"<p>test</p>>"},{"post_change_time":1547212198998,"post_content":"<p>new day, I really feel the need for a to-do list that stays in local storage.</p>\n\n<p>So let's check that out.</p>\n\n\n<p>I just started on refactoring to add the to-do list to this blog software, but it will not do to have that here.  I do not see a blog and a to-do list to have enough in common.</p>\n\n<p>I think we will have an eventual need for this blog software to render common components of this page in a reusable form, but the time is not yet.</p>\n\n<p>Anyway, now is a good time to put a little work into studying for the qualifying exam.</p>"},{"post_change_time":1547211807298,"post_content":"<p>Up late, so doing a little blogging while we have some down time.</p>"},{"post_change_time":1547186957142,"post_content":"<p>Watching too much of, \"The Wire,\" on Amazon.</p>\n\n<p>Just a few minutes of performance modeling for the qualifying exam...</p>\n\n"},{"post_change_time":1546786827902,"post_content":"<p>studying for the qualifying exam some more tonight</p>\n\n<p>I figure I have to put in a little each day.</p>\n"},{"post_change_time":1546783321752,"post_content":"<p>Good morning, on with the studying for the qualifying exam.</p>\n\n<p>I have gathered resources for COT 4420 - Formal languages and Automata Theory, I suppose I should move on to gathering resources for the next class.  The temptation is to stay focused on one class, but we have 5 more to get into.</p>\n\n"},{"post_change_time":1546782943782,"post_content":"<p>Hi everybody.  It's a new day, new post. </p>\n\n<p>OK, I realize I need to stop hiding from reality and start studying for the qualifying exam.  My only thing is I can't remember whether or not we submitted the request to take it!  That is massively embarrassing.</p>\n\n<p>OK, it is several hours later.  Speaking of not hiding from reality... I will do my private inventory now. When I come back I have a few things to comment on.</p>\n\n<p>OK, started a project board to track studying for the qualifying exam: <a href='https://github.com/jhancock1975/phd-thesis/projects/8'>Qualifying Exam Project Board</a></p>"},{"post_change_time":1546782812172,"post_content":"<p>Hi everybody.  It's a new day, new post. </p>\n\n<p>OK, I realize I need to stop hiding from reality and start studying for the qualifying exam.  My only thing is I can't remember whether or not we submitted the request to take it!  That is massively embarrassing.</p>\n\n<p>OK, it is several hours later.  Speaking of not hiding from reality... I will do my private inventory now. When I come back I have a few things to comment on.</p>\n\n<p>OK, started a project board to track studying for the qualifying exam: <a href='https://github.com/jhancock1975/phd-thesis/projects/8'>Qualifying Exam Project Board</a></p>"},{"post_change_time":1546745459938,"post_content":"<p>Hi everybody.  It's a new day, new post. </p>\n\n<p>OK, I realize I need to stop hiding from reality and start studying for the qualifying exam.  My only thing is I can't remember whether or not we submitted the request to take it!  That is massively embarrassing.</p>\n\n<p>OK, it is several hours later.  Speaking of not hiding from reality... I will do my private inventory now. When I come back I have a few things to comment on.</p>\n\n"},{"post_change_time":1546660330111,"post_content":"<p> Well the behavior of the blog is still inconsistent, but then so is the internet service provider.</p>\n\n<p>Things seem to be running more smoothly after restarting the router &amp; restarting the wireless NIC.  It could be that those things took just long enough for something completely beyond my control to get worked out.</p>\n\n<p>So now I feel like we have more control over creating and modifying posts, but the blog still lacks style and probably does not look good on a mobile device.  </p>\n\n<p>OK, I will create a to-do for Silenium testing.</p>\n\n<p>I bet there is a CSS style for text so it looks like a button.  I've been using  text like &lt;ctrl&gt;-x but I just saw some markdown that is better where it actually looked like a control button.</p>\n\n<p>I think I must switch to Think Stats now... I don't think the previous book is written well enough.  On top of that, we've got to start studying for the qualifying exam, now, and set up an appointment with the eye doctor and <b>do some chores around the house.</b></p>\n\n<h2> Think Stats </h2>\n\n<p> ...maybe not.  I just found this book: <a href='http://onlinestatbook.com/Online_Statistics_Education.pdf'>Introduction to Statistics</a>, that seems like a pretty good read.</p>"},{"post_change_time":1546659875906,"post_content":"<p> Well the behavior of the blog is still inconsistent, but then so is the internet service provider.</p>\n\n<p>Things seem to be running more smoothly after restarting the router &amp; restarting the wireless NIC.  It could be that those things took just long enough for something completely beyond my control to get worked out.</p>"},{"post_change_time":1546659818930,"post_content":"<p> Well the behavior of the blog is still inconsistent, but then so is the internet service provider.</p>"},{"post_change_time":1546659074420,"post_content":"<p> 2019-01-04 2231 </p>"},{"post_change_time":1546659068417,"post_content":"<p> 2019-01-04 2231 </p>"},{"post_change_time":1546658976633,"post_content":"<p> 2019-01-04 2229 </p>\n<p> Additional text </p>\n<p> Ok, I think we're functioning better, time to commit. </p>"},{"post_change_time":1546658179327,"post_content":"<p> 2019-01-04 2216 </p>"},{"post_change_time":1546657691895,"post_content":"<p> 2019-01-04 2143 </p>"},{"post_change_time":1546655310259,"post_content":"<p> test post 2019-01-04 2137 </p>"},{"post_change_time":1546655170100,"post_content":"<p> 2019-01-04 2126 </p>"},{"post_change_time":1546653333566,"post_content":"<p> test 2019-01-04 2055 </p>"},{"post_change_time":1546650657066,"post_content":"<p> 2019-01-04 2010 </p>"},{"post_change_time":1546650536123,"post_content":"<p> 2019-01-04 2008 </p>"},{"post_change_time":1546650494151,"post_content":"<p> test 2019-01-04 2007 </p>"},{"post_change_time":1546650415264,"post_content":"<p> test 2019-01-04  2006 </p>"},{"post_change_time":1546625744347,"post_content":"<p> 2018-01-04 1315 </p>"},{"post_change_time":1546610219612,"post_content":"<p> test 2018-01-04 0856 </p>"},{"post_change_time":1546609781768,"post_content":"<p> 2018-04-01 0849 </p>"},{"post_change_time":1546609705869,"post_content":"<p> test 2018-01-04 0848 </p>"},{"post_change_time":1546608788339,"post_content":"<p> test 2018-01-04 0832 </p>"},{"post_change_time":1546608684491,"post_content":"<p> test 2018-01-04 0831 </p>"},{"post_change_time":1546608467690,"post_content":"<p> test 2018-01-04 0827 </p>"},{"post_change_time":1546608008119,"post_content":"<p> test 2018-01-04 0820 </p>"},{"post_change_time":1546607937584,"post_content":"<p> test 2018-01-04 0818 </p>"},{"post_change_time":1546607415145,"post_content":"<p> test 2018-01-04 0810 </p>"},{"post_change_time":1546357749758,"post_content":"<p>I hope this push helps</p>"},{"post_change_time":1546357551491,"post_content":"<p>Ok, now we can edit  saved posts.  I don't know what happened to my last post, maybe I forgot the oauth token</p>"},{"post_change_time":1546357275184,"post_content":"<p>Allright, now we can start posts, and come back to them.</p>\n\n<p>But there's a conundrum to solve, and that is we have a synchronization problem, in that it takes a a minute or so for changes saved to be visible.</p>"},{"post_change_time":1546350436124,"post_content":"<p>I just had a thought, and that is that I should probably write some research papers, or ipynb notebooks like I have started for studying Statistics in order to prepare for the qualifying exam.</p>\n\n<p>Good morning, new day, so I will start a new post.</p>"},{"post_change_time":1546316133648,"post_content":"<p>It seems the update posts logic has a problem, as well as pagination - I see edited posts at the end of the list - we'll have to figure out a better solution, but I'm not so interested in working on that today.</p>\n\n<p>I am more interested in studying deep learning today, and I need to start thinking about studying for the qualifying exam.  I need to meet with my advisor. </p>\n\n<p>I still can't decide if I will continue with Emacs ansi-term for a shell.  I like it very much, but sometimes the scrolling gets a little messed up.</p>\n\n<p>OK, so here is another important thing to think about.  I amass all these links in this blog, but how do I get them back, and how can I make a reading list to get through?  I'm still on <u>Introduction to Statistical Learning</i> from last summer.  Anyway here is a link to <a href='https://www.youtube.com/watch?v=TjZBTDzGeGg&list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi'>MIT 6.034 Fall 2010</a>.  There should also be some lectures from 2013, but I am not finding them at the moment.\n\n<p>Taking anew tack with the <a href='http://www.stat.wmich.edu/s160/hcopy/book.pdf'>Kalamazoo book</a>.  Since some of the techniques feel a bit out-dated, I will find newer solutions in Python, even if they take a different approach than what the authors are suggesting. I am thinking of using techniques here: <a href='https://www.scipy-lectures.org/packages/statistics/index.html'>https://www.scipy-lectures.org/packages/statistics/index.html</a>.  Interestingly enough, this page has a reference to <a href='http://greenteapress.com/wp/think-stats-2e'>Think Stats</a>  which is an introduction to statistics that I wanted to check out earlier, but I had forgotten about.</p>\n\n<p>Wow, I didn't realize this site is so informative. <a href='www.scipy-lectures.org'>www.scipy-lectures.org</a>.  I'm actually going to save this post now, so I do not lose it.  This looks like a great site for doing statistics in Python.</p>"},{"post_change_time":1546259712373,"post_content":"<p>test change 0735 </p>\n<p>OK, today, I am back on the blog software, first thing I want to do is put in something for saving posts in individual files as well as in the master index.json.  Then we will add the capability for editing previously saved posts.</p>\n\n<p>Time to work locally, so saving post.</p>"},{"post_change_time":1546227115395,"post_content":"<p>OK, today, I am back on the blog software, first thing I want to do is put in something for saving posts in individual files as well as in the master index.json.  Then we will add the capability for editing previously saved posts.</p>\n\n<p>Time to work locally, so saving post.</p>\n\n<p>OK, now we can update posts, but it takes too long to see the changes reflect.</p>\n\n<p>Another day comes to a close.  I'm working more on the exercises from the Kalamazoo statistics book: <a href='http://www.stat.wmich.edu/s160/hcopy/book.pdf'>http://www.stat.wmich.edu/s160/hcopy/book.pdf</a>, but I'm finding the exercises a little tedious.  I am also trying to be rigorous in my citations, but this is a drag, too.  On the other hand, I do not feel right about not citing techniques I use from sources like StackOverflow. </p>"},{"post_change_time":1546197278226,"post_content":"<p>test 1414</p>\n<p>Still working on this titanic code, the next things is to have it generate a submission output for test.csv.  I'll create a to-do project and put that on there.</p>\n\n<p>Aside: Looks like \"m-x eshell\", or \"m-x ansi-term,\" are better alternatives to, \"m-x shell.</p>\n\n<p>Wow, ok, it looks like people can get 100% accuracy on the titanic data.</p>\n\n<p>I found random forest with default parameters gets 80%, and predicting by gender gets about 78%.</p>\n\n"},{"post_change_time":1546197129978,"post_content":"test 1412\nThe test 3"},{"post_change_time":1546193569405,"post_content":"test 1312"},{"post_change_time":1546177583997,"post_content":"<p>OK, today, I am back on the blog software, first thing I want to do is put in something for saving posts in individual files as well as in the master index.json.  Then we will add the capability for editing previously saved posts.</p>\n\n<p>Time to work locally, so saving post.</p>"},{"post_change_time":1546174133090,"post_content":"<p>Torn between working on the blog and the stats and the cloud deep learning stuff.</p>\n\n<p>Anyway here is the great keyboard shortcut for the day. For folding code in Atom, it is: &lt;ctrl&gt;-&lt;alt&gt;-[, and unfolding it is:  &lt;ctrl&gt;-&lt;alt&gt;-[ . </p>\n\n<p>Now, what was I doing?  Oh, yeah, looking for some Emacs way to SFTP. I remember about Emacs, there is an Emacs way to do everything. </p>\n\n<p>Ok, so there's something called TRAMP mode for Emacs for using SFTP, but I do not feel like getting that deep into it tonight.</p>\n\n<p>Ok, I'm going to try setting up a GPU instance on Google cloud now. Let's see if I run into the quota thing again.</p>\n\n<p>test 1027</p>"},{"post_change_time":1546122392409,"post_content":"<p>Right now, I'm archiving my home directory of my Amazon GPU instance so I can try another building of AWS instance with GPU support.  Maybe I should try Google cloud.</p>\n\n<p>Ok, I've been working some more on the exercises in: <a href='http://www.stat.wmich.edu/s160/hcopy/book.pdf'>Statistics and Data Analysis</a>.  My primary learning for today is how to do what I consider to be the modern version of a dot plot.  I see that this book is from 2001, so at that time they may have been stuck  using character based graphics for plotting their data.  A Seaborn graph from it's countplot function conveys the same information, with nice colors, so I prefer to use that instead.  Overall the text is really basic, but for some reason I feel like I need to get back to the basics with statistics.</p>"},{"post_change_time":1546087804508,"post_content":"<p>Yes, ok there should be two links on the end of a post. The first one is to edit, and should only be visible when the user enters a valid oath token.  The second one is a comment link.  That opens a can of worms as to adding comment functionality, but we should have it.  Ok, this should go on the to-do board, at least.</p>\n\n<p>I wonder if stem-leaf and dot-plots are out-of-date now.</p>\n\n<p>2018-12-28 0749, ok back at it.  Good morning.</p>"},{"post_change_time":1546055933972,"post_content":"<p>OK, I keep getting side-tracked with Emacs.  Somehow I am drawn to it, like a moth to a flame.  I have been ever since I found out about it, some time ago.  I remember hearing people say something like, \"Don't use that Emacs, it's too complicated...\"</p>\n\n<p>Interesting, I immediately decide it is time to rebuild TensorFlow from source, just as soon as I start working on the exercises in the statistics book: <a href='http://www.stat.wmich.edu/s160/hcopy/book.pdf'>http://www.stat.wmich.edu/s160/hcopy/book.pdf</a>.  OK, I'll do some exercises first.</p>\n\n<p>We've just discovered the wonderful world of stemgraphic since the authors mention that in the statistics book, and we are writing iPython notebooks to do the exercises.</p>\n\n<p>Looking for permalinks in StackOverflow - there is a subtle, \"share,\" link at the end of every answer, that has the permalink.  I had to use &lt;ctrl&gt;-f in my broswer to find it.</p>\n\n<p>I am taking a lot of time to formally write up the answers to the questions in this statistics book, and I am wondering if it is worth the time.  At least, we've learned to generate a stem-leaf plot.  Here is a link to the in-progress Jupyter notebook:<a href='https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions/blob/ch1/ch1/exercise-1.3.1.ipynb'>https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions/blob/ch1/ch1/exercise-1.3.1.ipynb</a>. Wow, I didn't know or I had forgotten that Github renders notebooks faithfully as they look in Jupyter.</p>\n\n<p>I just realized, probably the most important feature to add now is one that enables editing existing posts.  At the moment, this is not directly possible.  I am behind on documenting any functionality.  That is for sure.</p> "},{"post_change_time":1546049598229,"post_content":{}},{"post_change_time":1546000981993,"post_content":"<p>Hi, good moring</p>\n<p>I think now it is the time to put in pagination and build more static views of posts organized by dates.</p>"},{"post_change_time":1545966102441,"post_content":"<p>Ok, so maybe I am being stingy with the data - I figure the authors would not want me posting the baseball data publicly, the right thing would be to contact them and find out if they'd be ok with that.  Maybe I will do that.</p>  \n\n<p>Anyway, I have added any intermediate and final files related to extracting the baseball data to the .gitignore for the <a href='https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions.git'>https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions.git</a> repository.</p>"},{"post_change_time":1545965881932,"post_content":"<p>Ok, tesseract worked pretty nicely to get the baseball data to text format, just had to remove some blank lines after copy/pasting into a new file.  The revision from StackOverflow is: <a href='https://stackoverflow.com/posts/26478719/revisions'>https://stackoverflow.com/posts/26478719/revisions</a> by StackOverflow from October 21, 2014.  I am afraid to mention the user's name here, publicly.  If I was citing for a research paper, I would.</p>"},{"post_change_time":1545964288958,"post_content":"<p>ok, installed tesseract, seems like for dealing with this particular pdf, gost script is better than converting the pdf to tiff before ocr'ing it.  The syntax for the ghost script command is here.  The command line options are inscrutable to me: <a href='https://stackoverflow.com/posts/75567/revisions'>https://stackoverflow.com/posts/75567/revisions</a>.  I used the answer with edit by StackOverflow user BAR, from February 11th, 2016.<p>\n\n<p>While I'm wating for the ocr to finish, here's the command:</p>\n<pre>gs -q -dNOPAUSE -sDEVICE=tiffg4 -sOutputFile=a.tif foo.pdf -c quit</pre>\n<p>OK, OCR's done, let's get some data.</p>  "},{"post_change_time":1545963063938,"post_content":"<p>Ok, so </p>\n<ol>\n  <li>I really do not find much for sample proportion doing a google search.  I am thinking it probably goes by a different name, but it seems to me to be the ratio of the count of elements of a particular class in a distribution, divided by one less than the the number of elements in the distribution that are not of the class as the numerator.</li>\n<li>We've got to get mathjax working here</li>\n<li>We're goint to OCR <a href='http://www.stat.wmich.edu/s160/hcopy/book.pdf'>http://www.stat.wmich.edu/s160/hcopy/book.pdf</a>Since it seems like a scanned pdf and:\n<li>  <ol>\n    <li>They want us to work with this big dataset</li>\n    <li>If I can't copy &amp; paste it then it will take too long to deal with.</li>\n  </ol>\n</li>\n</ol>\n\n<p>for this perpose, I just installed tesseract. I wonder if that is still the best choice. </p>"},{"post_change_time":1545953960164,"post_content":{}},{"post_change_time":1545948962808,"post_content":"<p>Ok, starting on some statistics questions on a slow afternoon.  I was busy with mundane distractions this morning.</p>\n\n<p>I just realized, the easy way to use mathjaxs here is to use jupyter. Perhaps I will link to jupyter notebooks from here, or save them as html and then link to them, or maybe even frame them.</p>\n\n<p>Ok, off to another holiday dinner with family.</p>"},{"post_change_time":1545876145871,"post_content":"<p>well, I just updated this blog posting page, now the common areas are coming in double.  After finishing the header, footer and left column, I am wondering about the best way to do the center content.  If I leave this the way it is, we will have an html fragment referring to javascript components.  If we do that then we will have html files calling javascript and it won't be obvious where the source code for that javascript is.</p>"},{"post_change_time":1545875012092,"post_content":"<p>ok, I started a new repo at: <a href='https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions.git'>https://github.com/jhancock1975/StatisticsAndDataAnalysisSolutions.git</a> for working through problems in this statistics book.  I think it is pretty basic but it should fill in some gaps.</p>\n\n<p>Now I am working on factoring out common code.  Let's make sure the home page still loads</p>."},{"post_change_time":1545869380184,"post_content":"<p>Well good morning.</p>\n<p>That was easy.  We have submitted a csv to Kaggle.  However, it won't win, since it's nowhere near 100%</p>\n\n<p>So, what to study today? More of what is trendy? The latest in AI research?</p>\n\n<p>I've started checking recent submissions on archiv.  Looks like some new interesting material on neural modulation.</p>\n\n<p>I know that I need to tune hyperparameters for the titanic code - here seems like a decent treatise on the subject: <a href='https://github.com/TLESORT/Generative_Continual_Learning'>https://github.com/TLESORT/Generative_Continual_Learning</a> But I want something easy... like grid search for sklearn.</p>\n\n<p>Oh, and on another subject, the keras github repo has a lot of examples, and for that matter, the tensorflow one probably does, too.  Anyway, I'm setting up a pycharm project for those.</p>\n\n<p>I am starting to think I should go through the exercise of building tensorflow from source again, but... it is a lot of commands & I don't feel quite up to it at the moment.</p>\n\n<p>Or, perhaps what I should really do is find out how to start up a GPU instance on Google compute cloud.  The last time I tried I ran into some error about my quota.</p>\n\n<p>Ok, just made links on index page dynamic.  eventually this can be a one page site, using the react design pattern.  I'd rather not get into using all those libraries at this time.</p>"},{"post_change_time":1545830537578,"post_content":"<p>Still working on this titanic code, the next things is to have it generate a submission output for test.csv.  I'll create a to-do project and put that on there.</p>\n\n<p>Aside: Looks like \"m-x eshell\", or \"m-x ansi-term,\" are better alternatives to, \"m-x shell.</p>\n\n<p>Wow, ok, it looks like people can get 100% accuracy on the titanic data.</p>\n\n<p>I found random forest with default parameters gets 80%, and predicting by gender gets about 78%.</p>\n\n"},{"post_change_time":1545792925463,"post_content":"<p>back to titanic exercise</p>\n\n<p>df.describe gives NaN for unique counts when I think it shouldn't be.</p>\n\n<p>I've resorted to awk, but perhaps I should do some research.</p>\n\n<p>Nothing comes up  immediately, using google.</p>\n\n<pre> awk -F ... | sort | unique </pre>\n\n<p>Is working a lot better for me.</p>\n\n<p>I need to stop and get my IRC channel stuff working again. Ok, that really wasn't so bad.</p>\n\n<p>ok, just found something interesting.  I just did two commits to the <a href='https://github.com/jhancock1975/kaggle-titanic</a>https://github.com/jhancock1975/kaggle-titanic</a> repository.  The URI for cloning this repository is: <a href='https://github.com/jhancock1975/kaggle-titanic.git'>https://github.com/jhancock1975/kaggle-titanic.git</a>.  The two commits are:\n<ul>\n  <li><a href='https://github.com/jhancock1975/kaggle-titanic/blob/30b0b244674845edf3796f84f68ff7053c44116e/nn/titanic-nn.py'>30b0b244674845edf3796f84f68ff7053c44116e a Keras dense neural network to train on the Kaggle Titanic starter contest data (train.csv) that uses categorical cross-entropy that gets about 0.78 accuracy, and</a></li>\n  <li><a href='https://github.com/jhancock1975/kaggle-titanic/blob/1a609d76d9b70e9bae9199b1b5e927e1ca1b9c16/nn/titanic-nn.py'>1a609d76d9b70e9bae9199b1b5e927e1ca1b9c16 a Keras dense neural network to train on the same data that uses binary cross-entropy that gets about 0.38 accuracy.  Moreover, the accuracy does not seem to change all that much across epochs.</a></li>\n</ul>\n<p>I do not know why accuracy is so much lower for just changing the type of entropy, but I think it's a noteworthy result."},{"post_change_time":1545752512857,"post_content":"<p>Good morning.</p>\n\n<p>Working on the titanic starter-kaggle<p>\n\n<p>I wrote a baseline classifier using random forest that gets about 80%</p>\n\n<p>I wonder if it is about time to start using math jax here for equation typesetting.</p>\n\n<p>Dropout, after activation.  Makes sense, right?</p>\n\n<p>Wow.  I thought I would look at the origin of dropout in the paper: <a href=\"http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\">http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf</a>.  This paper is 30 pages.</p>\n\n<p>On a side note, I'm trying out using emacs for my shell. At work we are into using windows with split screens for our shells, emacs seems to be really good for having multiple shells with shells underneath, and is smarter about selecting with the mouse.  Tmux I can have windows underneath, but it feels sluggish, and the mouse selects across split windows.  Screen I can't find a way to have shell windows that are not visible if I am using a split screen, and selecting with the mouse selects on all visible shells.  Emacs suffers from none of these.  The only drawback seems that it doesn't play well with less out of the box.</p>\n\n<p>Oh, you know when you learn about something, and feel like a dolt because knowing what you just learned would have saved you a lot of time and anguish, and everyone else probably already knows this?  I just had one of those moments.  With vi on a mac, if I press option and then &lt;shift&gt;-: it's the same as typing &lt;esc&gt; &lt;shift&gt;-:. <p>\n\n<p>Ok, on data cleaning portion now, for random forest I just dropped anything not numerical, but now I want to keep more values.</p>\n\n<p>Going for a jog now, I'll post more later.</p>"},{"post_change_time":1545744381959,"post_content":"<p>For the blackjack challenge, I do not want to get too far into it because it is a toy problem.  From what I read in the discussion, and I am agnostic on the result, people are writing that ~ 43% is the max.  This gets about 41%:</p>\n\n<pre>\n if (player_total < 17) and (dealer_card_val <17):\n        return True\n</pre>\n\n<p>And here is my brute force for finding a linear combination of the parameters that finds the optimum pricing strategy:\n\n<pre>\nimport sys\nloop_range=range(-1,1)\nmax_result = sys.float_info.min\nbest_params = {'x1': 0, 'x2':0, 'x3':0, 'x4':0}\nfor x1 in loop_range:\n    for x2 in loop_range:\n        for x3 in loop_range:\n            for x4 in loop_range:\n                def get_pricing_function():\n                    def pricing_function(days_left, tickets_left, demand_level):\n                        \"\"\"Sample pricing function\"\"\"\n                        price = x1*days_left + x2*tickets_left + x3*demand_level + x4\n                        return price\n                    return pricing_function\n                result=simulate_revenue(days_left=7, tickets_left=50, pricing_function=get_pricing_function(), verbose=True)\n                if (result > max_result):\n                    max_result = result\n                    best_params = {'x1': x1, 'x2':x2, 'x3':x3, 'x4':x4}\n                print(\"result = %s\" % result)\nprint('max_result = %f' % max_result)               \nprint('best_params = %s' % best_params)\n</pre>\n\n<p>Ok, so what next?  I think it is important to keep abreast of what is happening on kaggle, what is trending on Github, Arxiv, ACM, etc.</p>\n\n<p>I'm trying a kaggle beginner contest now.  I think Kaggle competitions are probably one of the best ways to learn more about doing machine learning programming, but I am afraid of getting really sucked into some contest where I spend all my time working on that and nothing else.  This is what happen the last contest I was in at work.</p>\n\n<p>So anyway, I'm going to try doing a submission with the kaggle api now.  Looks like I can install it with just a pip package.</p>"},{"post_change_time":1545678557687,"post_content":"<p>For the blackjack challenge, I do not want to get too far into it because it is a toy problem.  From what I read in the discussion, and I am agnostic on the result, people are writing that ~ 43% is the max.  This gets about 41%:</p>\n\n<pre>\n if (player_total < 17) and (dealer_card_val <17):\n        return True\n</pre>\n\n<p>And here is my brute force for finding a linear combination of the parameters that finds the optimum pricing strategy:\n\n<pre>\nimport sys\nloop_range=range(-1,1)\nmax_result = sys.float_info.min\nbest_params = {'x1': 0, 'x2':0, 'x3':0, 'x4':0}\nfor x1 in loop_range:\n    for x2 in loop_range:\n        for x3 in loop_range:\n            for x4 in loop_range:\n                def get_pricing_function():\n                    def pricing_function(days_left, tickets_left, demand_level):\n                        \"\"\"Sample pricing function\"\"\"\n                        price = x1*days_left + x2*tickets_left + x3*demand_level + x4\n                        return price\n                    return pricing_function\n                result=simulate_revenue(days_left=7, tickets_left=50, pricing_function=get_pricing_function(), verbose=True)\n                if (result > max_result):\n                    max_result = result\n                    best_params = {'x1': x1, 'x2':x2, 'x3':x3, 'x4':x4}\n                print(\"result = %s\" % result)\nprint('max_result = %f' % max_result)               \nprint('best_params = %s' % best_params)\n</pre>\n\n<p>Ok, so what next?  I think it is important to keep abreast of what is happening on kaggle, what is trending on Github, Arxiv, ACM, etc.</p>\n\n<p>I'm trying a kaggle beginner contest now.  I think Kaggle competitions are probably one of the best ways to learn more about doing machine learning programming, but I am afraid of getting really sucked into some contest where I spend all my time working on that and nothing else.  This is what happen the last contest I was in at work.</p>\n\n<p>So anyway, I'm going to try doing a submission with the kaggle api now.  Looks like I can install it with just a pip package.</p>"},{"post_change_time":1545664785111,"post_content":"<p> Good morning.</p>\n\n<p>Ok, the command pallet is important in Jupyter - initially I saw something that said to use ctrl-shift-p.   That won't work in Firefox, so a little Googling found ctrl-shift-f, which works in Firefox.</p>\n\n<p>Speaking of Firefox, I'm really happy with how it has been working, and I have been using it for a Browser for about a week now.</p>\n\n<p>ok, Matrix factorization</p>\n\n<p>I'm not seeing how factorization comes into play in this section.  The main thrust of it seems to be that we take the dot product of the vectors that result from the embedding.  Maybe that is somehow equivalent to using factors of the embedding matrices, but I do not understand how that is.</p>\n\n<p>Gensim looks very interesting, especially for using an embedding after it is learned.</p>\n\n<p>Also good introduction to t-SNE in embeddings.  I remember plotting t-SNE plots from embeddings before, but I don't remember if I was using the scikit learn library for that.  This is a useful tool.</p>\n\n<p>Onto the micro challenges</p>\n\n<p>First one is about blackjack. </p>\n\n<p>On a side note, I just added a verification tag to this site's index.html page, we will see how results show up on Google.</p>\n\n<p>I would like to find a way to have the Google search engine index all of the postings on this.</p>"},{"post_change_time":1545625776016,"post_content":"<p>Well, apparently the post from the car did not make it</p>\n\n<p>I wonder why.</p>\n\n<p>On to the next training, which is: embeddings.</p>\n\n<p>The embeddings tutorial has important information on how to use the Keras functional API for models that have more than one input, which is probably a key piece I was missing when I was trying to add inputs to my GAN experiments earlier.</p>"},{"post_change_time":1545623706097,"post_content":"<p>Ok, back to kaggle tutorials, or doing something with the wife.   I hear a lot of exasperation in the background.</p>\n\n<p>More good stuff in the Machine Learning Explainability course, partial dependency plots, and the Python PDPBox  library.</p>\n\n<p>The partial dependence plots for single variables in the for loop are not showing for me.  We could google that, but why spend time fixing these workbooks?  I'm not inclined to do that here.</p>\n\n<p>I really like the Machine Learning Explainability course, but it packs a lot in and it deserves a few passes, or more reading on the same topics from another source.</p>"},{"post_change_time":1545583818800,"post_content":"<p> Well, it's been a busy morning, blogwise. This blog software is now a minimum viable product; people can use this with their github account, and they will not use any of their Github API quota when people view their index page.</p>\n\n<p>The next thing will be to refactor these pages so we are serving the common elements using fetch instead of serving duplicated code, which is what we are doing now. </p>"},{"post_change_time":1545582653622,"post_content":"test 1130 1 abcd"},{"post_change_time":1545581903707,"post_content":"I put a cool effect"},{"post_change_time":1545580657126,"post_content":"test 1057 abcd"},{"post_change_time":1545579681554,"post_content":"test abcd 1041"},{"post_change_time":1545579043550,"post_content":"test 1030 2 abcd"},{"post_change_time":1545579023595,"post_content":"test 1030 abcd 1"},{"post_change_time":1545579010646,"post_content":"test 1029 1 abcd"},{"post_change_time":1545572007340,"post_content":"test 0833 1"},{"post_change_time":1545571819059,"post_content":"test 0830 1"},{"post_change_time":1545570830892,"post_content":"test 0813 1"},{"post_change_time":1545570605941,"post_content":"<p> well the blog is in a state of flux now, I started refactoring so that we can save posts to a single file that gets loaded when the home page loads.</p>\n\n<p>It's about half done, but I think the changes are local thus far.</p>\n\n<p>We're on to the last section in the R tutorials now, to take a little break from coding.</p>\n\n<p>I want to let my subconscious work on the refactoring for a bit.</p>\n\n<p>Wow, ok, at least in the Kaggle Jupyter notebooks with the R kernel, hep(function_name) returns a really nicely formatted help page.  For example:\n\n<pre>help(str)</pre>\n\n<p><i>How quickly I forget!<i> In R str is for <i>structure</i>.  Nothing to do with <i>str</i>ings.</p>\n\n<p>Hmmm. I notice we have gone from mean absolute error to root mean squared error.  </p>\n\n<p>Ok, probably way too long to have found this out, but the best way to deal with these Kaggle notebooks when the commit button stops working is to use the cloud buttons to download and upload the notebook.  Saves time over copying &amp; pasting the cells...who would do that?</p>\n\n<p>I'm going to go out of order for the last three modules - I'm going to do this Machine Learning Explainability first, then embeddings, then the micro challenges.</p>\n\n<p>This is the first I've heard of Machine Learning Explainability, and the first my browser's spell checker has heard of it as well, it would seem...</p>\n\n<p>Wow, the machine learning explainability track starts off with some great information.  Intuitive and ... one of those things I feel like I should have thought of... shuffle a column of the validation data, and see if performance goes down.  If it does, we know the column is important.  Brilliant!</p>\n\n"},{"post_change_time":1545520348147,"post_content":"test 1812 1"},{"post_change_time":1545520290387,"post_content":"test 1811 1"},{"post_change_time":1545519992677,"post_content":"test 1806 1"},{"post_change_time":1545519919305,"post_content":"test 1805 1"},{"post_change_time":1545519830608,"post_content":"test 1803 1"},{"post_change_time":1545519076188,"post_content":"test 1751 1"},{"post_change_time":1545519024067,"post_content":"test 1750"},{"post_change_time":1545518985601,"post_content":"test 1749 1"},{"post_change_time":1545518857705,"post_content":"test 1747 1"},{"post_change_time":1545518807800,"post_content":"test 1746 1"},{"post_change_time":1545518764226,"post_content":"test 1754 1"},{"post_change_time":1545518664492,"post_content":"test 1744 1"},{"post_change_time":1545518469901,"post_content":"test 1741 1"},{"post_change_time":1545518386391,"post_content":"test post 1739"},{"post_change_time":1545518316040,"post_content":"test 1738 1"},{"post_change_time":1545518210752,"post_content":"test 1736 2"},{"post_change_time":1545518078245,"post_content":"test 1734"},{"post_change_time":1545517963941,"post_content":"test post 1732"},{"post_change_time":1545517812892,"post_content":"test 1730"},{"post_change_time":1545517722855,"post_content":"test 1728"},{"post_change_time":1545517675766,"post_content":"test 1727 2"},{"post_change_time":1545517648927,"post_content":"test post 1727"},{"post_change_time":1545517563951,"post_content":"test 1725"},{"post_change_time":1545517367264,"post_content":"test 1722"},{"post_change_time":1545517290211,"post_content":"test post 1721"},{"post_change_time":1545515992322,"post_content":"test 1659"},{"post_change_time":1545515928849,"post_content":"test 1658"},{"post_change_time":1545515756652,"post_content":"test 1655"},{"post_change_time":1545515691487,"post_content":"test 1654"},{"post_change_time":1545515587446,"post_content":"test 1653"},{"post_change_time":1545515499535,"post_content":"test 1651"},{"post_change_time":1545515431726,"post_content":"test post 1650"},{"post_change_time":1545515101799,"post_content":"test post 1644"},{"post_change_time":1545514960555,"post_content":"test post 1642"},{"post_change_time":1545514844168,"post_content":"test post 2 1640"},{"post_change_time":1545514811204,"post_content":"test post 1640"},{"post_change_time":1545514736433,"post_content":"test post 1638"},{"post_change_time":1545514588510,"post_content":"test blog post"},{"post_change_time":1545499000802,"post_content":"testing .then"},{"post_change_time":1545498390141,"post_content":"testing clear text box after clicking on save post"},{"post_change_time":1545498037581,"post_content":"<p>Well, the xgboost tutorial is ok but there were a lot of things I could not get to work, so I want to study those in detail from another source.</p>\n\n<p>Especially the function for plotting a decision tree- </p>\n<pre>xgb.plot.multi.trees</pre>"},{"post_change_time":1545497655229,"post_content":"<p>anything else</p>\n\"some quoted text\"\n</body>\n</html>"},{"post_change_time":1545497572895,"post_content":"</body>\n</html>"},{"post_change_time":1545497512186,"post_content":"<p>Good morning.  We continue with R level 2 - now we are learning about xgboost.</p>\n\n<p><strong>str() function in R</strong> - at least where tibbles or dataframes are concerned - is the <i>structure</i> function.  Contrast that with str() in Python, which is for converting things to <i>strings</i>.\n\n<p>Oh, ok, they threw this in on the side in the xgboost tutorial: we have a select_if function for dataframes, like select but we can do things like: </p> \n\n<pre>data_frame.select_if(is_numeric)</pre>\n\n</html>"},{"post_change_time":1545481514184,"post_content":"<p>That's a little better, I feel like I can see what I am writing.</p>\n\n<p>I have it on the to-do to use the ACE editor here.</p>\n\n<p>Ok, back to the R tutorial.</p>\n\n<p>Interesting, the R tutorial says Kaggle has everything in CRAN installed. </p>\n\n<p>Thus far level 2 R tutorial and the Siraj material, I think are the only tracks covering more advanced topics in machine learning.</p>\n\n<p>This will probably come in handy some time.  Here are lists of stop words in 19 different languages: <a href='https://www.kaggle.com/rtatman/stopword-lists-for-19-languages'>https://www.kaggle.com/rtatman/stopword-lists-for-19-languages</a>\n\n<p>Interesting technique, Tatman removes very common words hotel and room from hotel reviews for applying LDA.</p>\n\n<p>There are more links to text analysis topics at the end of the latent Dirichlet analysis and term frequency / inverse document frequency section of the R tutorials.</p>\n\n<p>Ok, onto the ggplot section...</p>\n\n "},{"post_change_time":1545425554522,"post_content":"<p>Back to the R tutorial</p>\n\n<p>Something interesting to keep in mind when working with these Jupyter notebooks with the R kernel: when I run a cell, and scroll down to some part of it (the cell has a lot of output), and then re-run the cell, the browser (FireFox) automatically scrolls to where I was, so this could be a source of confusion.</p>\n\n<p>I think I'm going to make this text area larger, it's really small right now.</p>"},{"post_change_time":1545424168000,"post_content":"<p>Entry for December 20th, 2018</p> <p>Starting off the daywith some more work on the blog software since we are starting early.</p> <p>Ok,been coding up the save blog post functionality for a while yet this morning,just found another really nice vim shortcut that I'm sure everyone alreadyknows, but I am writing about it to help it stick in my memory, and thisshortcut is hitting '*' in command mode, will jump to the next occurrence of aword under a string, almost as good as 'goto definition' in and IDE.</p> <p>Back to the Kaggle SQL tutorial...</p> <p> Something strange - it seems I amable to query the open air quality database in the forked notebook, but not inanother one that I created from a different exercise. There I am getting a 403,forbidden error, here not.</p> <p> So here's something, looks like BigTablesupports temp tables, which is something I do not think Hive does. For thatmatter, these BigTable queries seem to go a lot faster than Hive queries I'mused to, but that may also be because these tables are really not that big.</p><p>Just finished the SQL tutorial. There is not much there for anyone who hasalready been writing a lot of queries. For me, because I have not used an SQLflavor that allows these common table expressions (CTE's also known as temporarytables), this is good to know about.</p> <p> Predicting time series data is anice intro to long term/short term memories LSTM's that I do not understand.</p> <p>The other stuff on time series is a lot of code, and not a lot of commentary. The videos are well done, but I am still getting used to learning from videos.</p> <p> On to the R module. I see this one has two levels.\n<p> Notes for December 15th, 2018</p> <p>For my 5 day plus training, I will be studying the material in </p> <ul> <li><a href='https://www.kaggle.com/learn/overview'>https://www.kaggle.com/learn/overview</a></li> </ul> <p> If I have time I will also study:</p> <ul> <li><a href='http://karpathy.github.io/neuralnets/'>http://karpathy.github.io/neuralnets/</a></li> <li><a href='https://github.com/GokuMohandas/practicalAI'>https://github.com/GokuMohandas/practicalAI</a></li> </ul> <p>Useful things I learned from the Python intro:</p> <ul> <li>First lesson: <ul> <li>Python floor operator: //</li> </ul> </li> <li>Lesson on functions &amp; getting help: <ul> <li>can give int() function as key to min, <i>i.e.<i/>: min(string_1, string_2, string_3, key=int) </li> </ul> <li>Lesson on booleans: <ul> <li>Python will automatically convert booleans to integers: True + True = 2.</li> </ul> </ul>\n<p>put some html markup in there</p> <h1>Post it</h1>\ntest 2258\ntest 1\n<p>Entry for December 17th, 2018</p> <p>Starting off the day with Kaggle questions.</p> <p>The first thing I notice is that the commit utility is rather slow - so it seems like it always takes a few minutes to save one's work when using the Kaggle exercise system.</p> <p>Today I would like to continue doing the Kaggle exercises, as well as some of the items in the to-do list.</p> <p>The project board is a really great tool in this Wiki for planning out work.</p> <p>Here is a good reference for desigining objects in JavaScript: <a href='https://crockford.com/javascript/private.html'>https://crockford.com/javascript/private.html</a> </p>Wow! really nice, just paste the commit SHA into a note in a Github project note on the kanban board, and it becomes a hyperlink!</p> <p>Ok, done adding some object orientation to the blog software</p> <p>In the slot machine demo - I didn't think to subtract the cost of playing the slot machine itself. Also, interesting opportunity to apply the law of large numbers. The experiment is computing the average cost of playing the slot machine, and repeating it multiple times and averaging the outcome of the experiment will give a value that approaches the true expected average cost.</p> <p>Something to note, that one might get lazy about with Python is we don't have to worry about String.equals, I can compare strings with str1==str2</p> <p>Interesting construct I never heard of: dictionary comprehensions, syntax is similar to list comprehensions, but using {}'s instead.</p> <p>Well, I feel a little small about this, but, at least for the Pandas data frame that the describe() function returns, the syntax to access an element of that description data frame - say the data frame is named df - df['column_name']['row_name']. I am used to thinking of 2-dimensional arrays as accessed by [row][column], so that's the transpose of what I am used to.</p> <p>The machine learning tutorial is very basic, best for someone who knows zero about machine learning.</p>\ntest 2255\ntest 2303\ntest text\nas they say on slashdot, first post\n<p>Entry for December 19th, 2018</p> <p>Now, it seems we get to the good stuff - things I've never really heard about before. Transfer learning. Take a trained model, replace the last layer with something new, and train just that last layer. This is extremely useful.</p> <p>Lesson 4 of the deep learning tutorial covers this.</p> <p>Interesting, it seems the example code is importing from Keras libraries within TensorFlow, not Keras by itself.</p> <p>Interesting note from lesson 6 in the Deep Learning track: Becker says, models with dense layers for tabular data, models with convolutional layers for images. Why?</p> <p>Batch size = # of rows (tabular data), or # of images (image data)</p> <p>'...use validation scores as ultimate measure of model quality...' - Dan Becker, Kaggle Deep Learning Track, Lesson 7</p> <p>Really important side note: in vim, in whatever the non-insert mode is called, I think it is called command mode, but if you have the cursor on the curly brace or whatever and you type '%' it will jump to the closing, and oh, wow, ctrl-o goes to the previous location, and ctrl-i or &gt;tab&lt; goes to next location. Those are huge.</p> <p> Ok, on to the data visualization module. The deep learning module is a bare introduction. </p> <p> The data visualization model is, I think best as a reference to see what commands create which types of charts. </p> <p> As an aside, how to extract certain columns:</p> <pre> df=pd.DataFrame({'col1':[1,3,4,5], 'col2':[1,2,3,4], 'col3':[1,4,5,6]}) df.loc[:,['col1','col3']] </pre> <p> In the data visualization track, in the seaborn section, there is a good explanation of box plots. I did not know, or I have forgotten that half the data is in the box. </p> <p> Ok, so I learned something really good from the visualization tutorial - the plotly library is easy to install and use in python now. For some reason I was thinking one needed to get an API key from plotly to use the library, but that is not the case. I just set up a virtual environment with plotly, pandas, and jupyter, and I can do the same plots that are in the tutorial.</p> <p> The other thing that is cool about plotly, that I didn't even know what they are called is choropleths, which are visualizations like maps with countries colored in according to some statistic.</p> <p> Onto their SQL tutorial, it's using google big query, which is interesting. I think Google is offering an alternative to hadoop and especially hive here, maybe HBase + Phoenix too. I wonder which it is closer to.</p>\n<p>Well, I just learned something cool that I can write htmlon the  items in the Github project kan-ban and that works.</p> <p>I am workingon the blog software again now.</p> <p>We are going back to the idea of savingthe posts as json data.</p><p>Last night I realized it kind of doesn't makesense to search the commits totry and figure out when a post is committed.</p>\ntest 2252\n<p>Entry for December 18th, 2018</p> <p>Today, on the GitBlog side I will put in the post functionality, I think.</p> <p>Another thought is I should probably try twitter bootstrap css</p> <p>Right now, I'll continue on the Pandas tutorial on the Kaggle site.</p> <p>Here is a nice one-liner for reading a dataframe from sqlite3:</p> <pre> music_reviews = pd.read_sql_query('select 1545321590990.html 1545321590990.json 1545322276130.html 1545322276130.json 1545322405355.html 1545322405355.json 1545322465365.html 1545322465365.json 1545364170261.html 1545364170261.json 1545364369876.html 1545364369876.json 1545364559794.html 1545364559794.json 1545364707969.html 1545364707969.json 1545364815617.html 1545364815617.json 1545364992845.html 1545364992845.json 2018_12_15.html 2018_12_15.json 2018_12_16.html 2018_12_16.json 2018_12_17.html 2018_12_17.json 2018_12_18.html 2018_12_18.json 2018_12_19.html 2018_12_20.html 2018_12_21.json jsonify-posts.sh from artists', sqlite3.connect('../input/pitchfork-data/database.sqlite')) </pre> <p>And here is how to set the values of indices:</p> <pre> animals = pd.DataFrame({'Cows': [12, 20], 'Goats': [22, 19]}, index=['Year 1', 'Year 2']) </pre> <p>Select the first row of a dataframe df:</p> <pre>df.iloc[0]</pre> <p>First 10 values of a column:</p> <pre> df.column_name.iloc[:10]</pre> <p>Pick and choose rows:</p> <pre>df.iloc[[1,2,3,5,8]]</pre> <p>Still doing the Pandas tutorial, this is the correct answer for number 7:</p> <pre>df = reviews[['country', 'variety']].iloc[0:100]</pre> <p>Which seems to me to be the opposite of what they just quoted in the documentation</p> <p>Another good point about doing the Kaggle tutorials is that it teaches using the Kaggle environment, which is basically Jupyter, but with bells &amp; whistles that Kaggle has added on.</p> <p>Median value of dataframe df's column_name column:</p> <pre>df.describe().column_name['50%']</pre> <p>Wow, didn't know this would work. Subtract mean value of column with name column_name from every value in column_name, using some dataframe named df:</p> <pre>mean_column_name_values = df.column_name - (df.describe().column_name['mean'])</pre> <p>It seems like:</p> <pre>df.column_name.value_counts()</pre> <p>and</p> <pre>df.groupby('column_name').size()</pre> <p>Get the same data, but it is organized differently. It seems like the point of it in the tutorial is that </p> <pre>df.groupby('column_name').size() creates a series with indices that are values we are counting. </pre> <p>df.groupby(['column_1', 'column_2']) creates a multi-index</p> <p>Important syntax note for dataframes - one can reference a column, and then apply a function to every value in the column, like so:</p> <pre>df.column_name.function()</pre> <p>I think the function must be a method of the Pandas Series object.</p> <p>I was thinking of stealing some example code for implementing commit and push for the blog, but it appears to be a dead end - there is some mature code for using the Github REST API in JavaScript, but I would end up adding thousands of lines of code to get that functionality - lots of dependent libraries, so in the end it will probably be better to follow the tutorial and do the steps the author recommends, but write my own functions, referencing the github API documentation.</p> <p> I am trying kaggle in FireFox. It seems chrome is unstable for this. Also, it chrome flickers when committing, which is really not good.</p> <p> I think Kaggle itself, might be kind of busy, but it looks like the commit dialog box that opens does not cause the page to flicker in FireFox, which is wonderful. </p> <p> Just learned something important - pre-trained models are available on Kaggle. For example, Resnet50.</p> <p> Ok, maybe I got too excited there, looks like they're possibly taking credit for some models built in to TensorFlow. Oh, maybe not, I think maybe they have weights stored for us to use.</p>\ntest 2300\ntest 2248\n<p>December 16th, 2018</p> <p>This morning, I went off on a tangent. Github has a rest API. I added some proof-of-concept code to this page in the js/site_functions.js file. It should be possible to read and write blog posts using the Github rest API. The proof of concept code is now setting the contents of the div with id blog_posts at the top of this page. </p> <p>Back to the Kaggle tutorials, onto a review of lists in Python. <p>Things were going great with the Github API blog project, until I blew my quota. I did not know unauthenticated requets are limited to 60 per hour per IP address.</p> <p>Interesting things from Kaggle tutorial:</p> <ul> <li>Lists tutorial:</li> <ul> <li>Python numbers have a .as_integer_raio() method that returns the numerator and denomenator.</li> </ul> </ul> <p><i>2305</i> I feel like I might have gone overboard with the blogging software today &semi; I meant to work on this slowly, over a longer period of time, but it is quite addictive.</p> <p>So, anyway, now we can write blog posts and retrieve them, but there's no limit on the number of posts we might show, and we need to add some error message if the user is over the quota</p> <p>Which brings me to the second point, this blog software really does not work for mass consumption, at least at present because after a few refreshes, the account that the github.io project that hosts the blog will get rate limited.</p> <p>What we need is to statically render the page that displays the posts - probably when a new post is added.</p> <p>The page where one can add a post should be oath token protected. <p>We also need to add a user-friendly error message when the account is over quota.</p>"},{"post_change_time":1545423233898,"post_content":"<p>Level 2 of the R tutorial starts off with a nice link: to an online book: <a href='https://r4ds.had.co.nz/'>R for Data Science</a> \n\n<p>I think that before I go any further, I will concatenate all the old posts and put them in one new post</p>"},{"post_change_time":1545422947953,"post_content":"<p> Ok, finallly getting to the R tutorial on Kaggle.</p>\n\n<p>Ok,  so R has these tibble objects that are like tables.  I wish it were called something else.</p>\n\n<p>That is interesting,when I start the R practice notebook, I get a disclaimer dialog box about competitions.</p>\n\n<p>Some of the material in the R machine learning tutorial seems to be almost a word-for-word copy of the material in the Python machine learning tutorial.</p>\n\n<p>Ok, on to level 2 of the R tutorial.  It was mostly a repeat of level 1.</p>\n\n<p>I am going to save this blog post and see what happens.</p>"},{"post_change_time":1545414308478,"post_content":"<p>what happens if I use html</p>\n<ul><li>test</li></ul>"},{"post_change_time":1545414242785,"post_content":"Well, so much for working on the Kaggle tutorials this morning.\n\nI spent most of my time on this blog software, and now it is pretty much where I want it.\n\nThe only thing, I suppose is maybe put in some option to show older messages.\n\nAlso, I notice it doesn't pull newer messages like I would like. But it's a start, I'll start touting it."},{"post_change_time":1545413753679,"post_content":"Test I love Sabrina 1235"},{"post_change_time":1545413502185,"post_content":"test from google chrome"},{"post_change_time":1545412751945,"post_content":"test 1219 $$$"},{"post_change_time":1545412703025,"post_content":"test 1218"},{"post_change_time":1545412641529,"post_content":"test 1217"},{"post_change_time":1545412040852,"post_content":"test 1207"},{"post_change_time":1545411813198,"post_content":"test 1203"},{"post_change_time":1545411143027,"post_content":"test 1152"},{"post_change_time":1545410863295,"post_content":"test post 1147"},{"post_change_time":1589124622278,"post_content":"<p>I had almost forgotten about this blog.\n<p>Working on a conference paper today.\n<p>RTFM man! Just found out about more we can do with RefTeX - it is quite valuable.  I wonder what we could have that is similar for writing HTML documents.  I am quite sure academic writing needs to move to HTML...or web pages need to all become PDF's... it seems like we do not need both.."}]